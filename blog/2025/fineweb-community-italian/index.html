<!DOCTYPE html> <html lang="it"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fineweb-Community - Analisi e Estrazione Topic | Oscar Pindaro </title> <meta name="author" content="Oscar Pindaro"> <meta name="description" content="Un framework per convertire personaggi in Language Models"> <meta name="keywords" content="Artifical Inteligence, Deep Learning, Gaming, Game Design, CNN,LLM, portfolio-website"> <meta property="og:site_name" content="Oscar Pindaro"> <meta property="og:type" content="article"> <meta property="og:title" content="Oscar Pindaro | Fineweb-Community - Analisi e Estrazione Topic"> <meta property="og:url" content="https://OscarPindaro.github.io/blog/2025/fineweb-community-italian/"> <meta property="og:description" content="Un framework per convertire personaggi in Language Models"> <meta property="og:locale" content="it"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Fineweb-Community - Analisi e Estrazione Topic"> <meta name="twitter:description" content="Un framework per convertire personaggi in Language Models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?e19b909c603d80f6dfde4781dffed50c" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oscarpindaro.github.io/blog/2025/fineweb-community-italian/"> <script src="/assets/js/theme.js?585d9cfe244bf6c2b137438c5bcc09b8"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Oscar</span> Pindaro </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Progetti </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fineweb-Community - Analisi e Estrazione Topic</h1> <p class="post-meta"> Created in February 23, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/gen-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> gen-ai</a>   ·   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   <a href="/blog/category/transformers"> <i class="fa-solid fa-tag fa-sm"></i> transformers</a>   <a href="/blog/category/generative-ai"> <i class="fa-solid fa-tag fa-sm"></i> generative-ai</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Allo stato attuale, la lingua principale parlata dagli LLM è l’inglese, mentre altre lingue sono molto più marginali e spesso trascurate dai creatori di modelli.</p> <p>La tabella qua sotto riporta il numero di dataset e modelli NLP disponibili attualmente su <a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">HuggingFace</a>. Ovviamente, la lingua inglese regna su tutti, con più di 15 mila dataset e 150 mila modelli allenati su diversi domini (generale, biomedico, legale, etc.). La lingua italiana vede invece una rappresentazione molto più magra, addirittura sotto tutte le altre principali lingue europee.</p> <table> <thead> <tr> <th style="text-align: left">Lingua</th> <th style="text-align: center">Dataset</th> <th style="text-align: center">Modelli</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Italiano</td> <td style="text-align: center">660</td> <td style="text-align: center">6,254</td> </tr> <tr> <td style="text-align: left">Inglese</td> <td style="text-align: center"><strong>16,746</strong></td> <td style="text-align: center"><strong>154,599</strong></td> </tr> <tr> <td style="text-align: left">Francese</td> <td style="text-align: center">1,404</td> <td style="text-align: center">8,951</td> </tr> <tr> <td style="text-align: left">Spagnolo</td> <td style="text-align: center">1,167</td> <td style="text-align: center">8,092</td> </tr> <tr> <td style="text-align: left">Tedesco</td> <td style="text-align: center">943</td> <td style="text-align: center">8,004</td> </tr> </tbody> </table> <p>I grandi modelli privati (come ChatGPT e Claude) conversano abbastanza agilmente in italiano, mentre quelli open-source sono ancora acerbi. In particolare, manca una buona alternativa computazionalmente efficiente che possa conversare in italiano su hardware modesti.</p> <p>La penuria di dataset open in lingua italiana rende lo sviluppo di modelli migliori poco conveniente e razionale, e le BigTech come Meta, Google e OpenAI preferiscono ottimizzare e sviluppare sulla lingua più parlata in Occidente.</p> <h2 id="fineweb-edu">Fineweb-Edu</h2> <p>Nel maggio 2024 HuggingFace ha rilasciato Fineweb <a class="citation" href="#fineweb">(Penedo et al., 2024)</a>, un dataset di 15 trilioni (!!!) di token in lingua inglese. Il dataset è stato creato a partire da <a href="https://commoncrawl.org/" rel="external nofollow noopener" target="_blank">CommonCrawl</a>, un dump di tutti i contenuti presenti su internet fino a quel momento. Questo dataset contiene il codice sorgente di moltissime pagine web, e quindi HuggingFace ha dovuto innanzitutto trasformarlo in un formato leggibile da LLM e esseri umani.</p> <p>Tuttavia, non basta avere accesso a questi dati. Internet infatti è pieno di siti web spam, di bassa qualità, generati automaticamente, e così via. Nel report è riportato dettagliatamente come HuggingFace ha ripulito il dataset originario, concentrandosi su due aspetti principali:</p> <ul> <li> <strong>Deduplicazione</strong>: è un processo che si occupa di scartare contenuto ridondante. Questo viene fatto per evitare che contenuti presenti in molte copie monopolizzino i parametri dei modelli durante l’allenamento;</li> <li> <strong>Qualità</strong>: parte del contenuto risulta avere problemi di formattazione, testo generato automaticamente da bot, miscugli di lingue incoerenti. Questi campioni devono essere buttati visto che avrebbero su qualsiasi training un contributo estremamente negativo.</li> </ul> <p>Insieme a questo dataset, è stata anche pubblicata una sotto-porzione di 1.3 trilioni di token chiamata <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="external nofollow noopener" target="_blank">Fineweb-Edu</a>. Questo dataset ha un focus su contenuti educativi, come blogpost, paper, lezioni universitarie.</p> <p>Per poter estrarre questi campioni di alta qualità, HuggingFace ha annotato un subset di Fineweb con Llama3-70B-Instruct, dividendo i campioni per contenuto e qualità informativa. Da queste annotazioni ha costruito poi un classificatore molto più snello e rapido che si occupa di classificare su larga scala il resto del dataset.</p> <p>La lingua principale di Llama è l’inglese, e quindi questo filtraggio non funziona molto bene su campioni in altre lingue, in quanto le performance di classificazione sarebbero molto più basse.</p> <p>Per superare questa limitazione dei modelli generativi attuali, è stato quindi creato Fineweb-C.</p> <h2 id="fineweb-c">Fineweb-C</h2> <p><a href="https://huggingface.co/datasets/data-is-better-together/fineweb-c" rel="external nofollow noopener" target="_blank">Fineweb-C</a> può essere considerato il naturale passo successivo a FineWeb, estendendo l’analisi fatta in precedenza su tutte le restanti lingue (compreso il napoletano). L’idea di base è sempre la stessa: trovare un modo per distinguere campioni di bassa qualità da campioni informativi. Se in FineWeb si usava un LLM, per questo dataset invece HuggingFace ha chiesto aiuto alla community. Per ogni lingua sono stati estratti 1000 campioni, che sono stati e stanno venendo annotati da dei volontari fluenti in quella lingua.</p> <p>In particolare, gli annotatori devono classificare i campioni nelle seguenti classi:</p> <ul> <li> <strong>None</strong>: il campione non ha alcun contenuto informativo. Può essere pubblicità, spam, un post su un social che non parla di niente di particolare, news di gossip;</li> <li> <strong>Minimal</strong>: il campione contiene un contenuto informativo molto minimo, come ad esempio un articolo di giornale che parla di un particolare evento senza dilungarsi troppo;</li> <li> <strong>Basic</strong>: il campione ha un principio di intento educativo, come una definizione, una breve spiegazione;</li> <li> <strong>Good</strong>: il testo è una spiegazione abbastanza dettagliata, anche ben formattata. La maggior parte dei concetti sono esposti chiaramente;</li> <li> <strong>Excellent</strong>: il testo è di qualità sopraffina, molto ben formattato. Un esempio potrebbe essere un blogpost molto tecnico oppure una dispensa universitaria;</li> <li> <strong>Problematic Content</strong>: questa è una categoria ombrello in cui finiscono tutti quei test formattati male, oppure spam, pornografia, materiale sensibile, etc.</li> </ul> <p>Lo split italiano è stato, ad oggi, annotato da 26 annotatori. Io personalmente ho contribuito annotando circa 400 campioni. Questi 1000 campioni sono stati pescati casualmente dal dataset originale, pre-filtrando materiale problematico. Devo dire che la pipeline di pre-filtraggio ha funzionato molto bene, visto che non mi è capitato quasi mai di trovare campioni imbarazzanti. Ho letto che non è stato così semplice in altri casi, specialmente per le lingue del sud-est asiatico, in cui la pipeline ha fatto passare moltissimo contenuto esplicito.</p> <h3 id="perché-è-importante">Perché è importante</h3> <p>Come già detto prima, l’Italiano è una lingua molto poco rappresentata. Quando cerco modelli per il lavoro di tutti i giorni, sono sempre costretto a scegliere LLM generici allenati su dataset che iniziano a mostrare i segni del tempo. Provo una grande invidia per chi sviluppa in inglese, che ha addirittura classificatori e retriever già pronti per il campo medico o altri campi estremamente specifici.</p> <h3 id="criticità">Criticità</h3> <p>Mi sembra onesto parlare anche dei punti un po’ più dolorosi della questione. Mentre annotavo il dataset, ho avuto il sospetto di star leggendo materiale coperto da copyright. La proprietà intellettuale è sempre un argomento caldo su cui c’è molta ipocrisia. Ci si interessa solo se si è vittime del fenomeno, mentre è facile ignorarlo se non si è un creatore di contenuti. Questo è ovviamente un tema molto ampio che inizia da ben prima dell’avvento dell’AI generativa.</p> <p>Probabilmente cambierò idea altre 100 volte su questo. Attualmente penso che la natura open di questo progetto lo allevia un po’ dalle inevitabili colpe di cui si macchierà quando verrà scalato sull’intero dataset. Spero però che parlandone il grande pubblico possa iniziare a capire un po’ meglio l’origine di questi dati e inizi una conversazione attorno a questo argomento.</p> <p>C’è poi ovviamente tutto un tema secondario sul fatto che i modelli generativi vengono utilizzati molto per generare fake news e scam. Di sicuro rendere gli LLM fluenti in multiple lingue aumenterà il raggio di questa piaga, ma almeno per design questo dataset prova a raccogliere campioni con contenuto educativo.</p> <h2 id="analisi-del-dataset">Analisi del dataset</h2> <p>Mentre annotavo il dataset, mi è sembrato che i campioni di alta qualità fossero soprattutto in ambito teologico e politico. Ho infatti trovato molti testi estratti dal Catechismo e da riflessioni sul capitale di Marx.</p> <p>Per questo motivo, ho deciso di fare un’analisi del dataset per vedere la distribuzione dei topic. L’analisi è strutturata nel seguente modo:</p> <ol> <li>estrazioni di parole chiave dal testo;</li> <li>utilizzo di un LLM per estrarre un topic generale;</li> <li>visualizzazione dei dati e considerazioni.</li> </ol> <p>Il codice sorgente lo trovate qua <a href="https://github.com/OscarPindaro/fineweb-c-analysis-ita" rel="external nofollow noopener" target="_blank">nella mia repo</a>.</p> <h2 id="estrazione-delle-parole-chiave">Estrazione delle parole chiave</h2> <p>Visto che ho intenzione di usare un LLM per estrarre dei topic di alto livello dal testo (come ad esempio “Moda”, “Tecnologia”, “Teologia”), ho deciso innanzitutto di estrarre delle parole chiave per ogni campione del dataset. Le parole chiave sono presenti all’interno dei campioni, e quindi io le considero di <em>basso livello</em> in quanto sono spesso molto specifiche per il testo analizzato. Questo passo è fondamentale perché mi permette:</p> <ul> <li>di conoscere meglio i contenuti del dataset;</li> <li>di capire che analisi devo fare;</li> <li>di dare informazioni aggiuntive all’LLM per aiutarlo nel suo lavoro. Purtroppo non sono riuscito a fare nessuno studio di ablazione che controlli quanto queste parole chiave aiutino l’LLM, ma mi sembra una intuizione ragionevole e comunque le avrei estratte in ogni caso.</li> </ul> <p>Per trovare le parole chiave ho usato <strong>TF-IDF</strong> <a href="https://it.wikipedia.org/wiki/Tf-idf" rel="external nofollow noopener" target="_blank">(Term Frequency - Inverse Document Frequency)</a>. L’intuizione dietro a questo algoritmo è la seguente: una parola non è importante se è molto o poco presente in termini assoluti, ma è importante se è presente solo in questo particolare campione e assente negli altri. Questo permette di scovare per ogni documento le parole che lo identificano unicamente, considerando la loro frequenza assoluta ma anche la frequenza all’interno del singolo testo. Ad esempio, se ho due documenti diversi, uno che descrive la vita di uno scienziato, e un altro che descrive la vita dello scienziato Enrico Fermi, per entrambi la parola <em>scienziato</em> è molto importante, ma per il secondo le parole “Enrico” e “Fermi” lo rendono unico.</p> <p>Ecco degli esempi di parole chiave che ho estratto:</p> <ul> <li>dal campione 1 ho estratto le parole chiave <code class="language-plaintext highlighter-rouge">['fedi', 'oro', 'anello', 'fondere', 'anelli']</code>, quindi probabilmente parlerà di <em>Matrimonio</em>, o comunque <em>Religione</em>;</li> <li>dal campione 12 ho estratto le parole chiave <code class="language-plaintext highlighter-rouge">['plusvalore', 'capitale', 'produzione', 'merce', 'accumulazione']</code>, quindi probabilmente parlerà di <em>Economia</em> (o di <em>Marxismo</em>).</li> </ul> <p>Per questo algoritmo è molto importante fare un po’ di data cleaning, rimuovendo articoli, preposizioni, numeri e altre parole ad alta frequenza ma poco interessanti.</p> <h2 id="estrazione-dei-topic-di-alto-livello">Estrazione dei Topic di alto livello</h2> <p>Una volta estratte le keyword, bisogna estrarre i topic generali. Per questi, ho deciso di utilizzare Llama3.1 quantizzato a 8 bit (<code class="language-plaintext highlighter-rouge">llama3.1:8b-instruct-q8_0</code>) e Gemma 2 (<code class="language-plaintext highlighter-rouge">gemma2:2b</code>), visto che riesco a lanciarli sulla mia <strong>RTX 4070 12GB</strong>. Ho anche provato a utilizzare una versione distillata di Deepseek (<code class="language-plaintext highlighter-rouge">deepseek-r1:8b-llama-distill-q8_0</code>), ma come mostrerò più avanti i tempi di calcolo erano un po’ troppo lunghi e ho preferito ignorarlo. Il prompt di sistema è strutturato nella seguente maniera:</p> <ul> <li>spiegazione delle classi di qualità di Fineweb-C;</li> <li>Indicazioni su quali informazioni il modello ha accesso (elenco di parole chiave, testo);</li> <li>Regole di categorizzazione: il modello deve dare categorie di alto livello e non specifiche. Ha accesso ad una lista di categorie pre-calcolate, ma può comunque scegliere di assegnare una nuova categoria non esistente;</li> <li>Formato dell’output: il modello deve scrivere tutto all’interno di tag xml, visto che sono semplici da parsare.</li> </ul> <p>Il modello ha la libertà di scegliere una classe anche se non è presente tra quelle fornite. Questo mi permette di avere un po’ di flessibilità, anche perché è un dataset che conosco poco. Operativamente, quando il modello sceglie una classe non presente tra quelle esistenti, viene aggiunta alla lista delle classi possibili, condizionando le future generazioni.</p> <p>Alla fine di questo processo, Llama ha estratto circa 142 topic. Ci sono alcuni casi in cui alcune categorie sono duplicate (singolare/plurale) ma per la visualizzazione di seguito non avrà molto peso.</p> <p>Gemma invece ha estratto 94 categorie, ma ho preferito quelle estratte da Llama.</p> <p>Per servire Llama e Gemma ho usato <a href="https://ollama.com/" rel="external nofollow noopener" target="_blank">ollama</a>. L’ultima versione di ollama ha introdotto il <code class="language-plaintext highlighter-rouge">prompt caching</code>, che permette di mantenere una cache delle ultime richieste fatte al modello. Nel caso di prompt molto simili tra una sessione all’altra, questa cache viene utilizzata per evitare di ricalcolare tutti i valori dell’attenzione del prompt di sistema, aumentando drammaticamente la velocità di generazione. Questo mi ha permesso di avere i topic generali in una decina di minuti. Purtroppo con DeepSeek questo vantaggio non si è presentato: poiché è un modello “reasoning”, prima di dare una risposta, emette molti token di ragionamento, e il tempo di annotazione quindi è aumentato da 10 minuti a 4 ore.</p> <p>Ancora una volta sono rimasto molto stupito dalle performance di Llama, mentre Gemma non mi ha reso particolarmente entusiasta.</p> <details><summary>Prompt di sistema - Template Jinja</summary> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sei un assistente specializzato nell'analisi e classificazione di testi in italiano. Il tuo compito è duplice:
<span class="p">
1.</span> Comprendere il livello qualitativo del contenuto informativo del testo, basandoti sulla seguente scala:
<span class="p">
   -</span> Problematic Content: contenuti inappropriati (pornografia, gambling, testo mal formattato)
<span class="p">   -</span> None: assenza di contenuto informativo (es. pubblicità, post social)
<span class="p">   -</span> Minimal: contenuto con minima valenza informativa non intenzionale
<span class="p">   -</span> Basic: contenuto con discreto valore informativo
<span class="p">   -</span> Good: contenuto ben strutturato con chiaro intento educativo
<span class="p">   -</span> Basic: contenuto con elevato valore informativo e ottima strutturazione
<span class="p">
2.</span> Identificare una categoria tematica di alto livello che rappresenti l'argomento principale del testo.

CONTESTO OPERATIVO:
<span class="p">
-</span> Hai accesso a un elenco di parole chiave di basso livello estratte dal testo
<span class="p">-</span> Hai accesso a un elenco di categorie tematiche già utilizzate in precedenza
<span class="p">-</span> Puoi sia utilizzare categorie esistenti che crearne di nuove quando necessario

REGOLE DI CATEGORIZZAZIONE:
<span class="p">
-</span> Usa categorie ampie e generali (es. "Medicina", "Sport", "Tecnologia")
<span class="p">-</span> Mantieni consistenza con le categorizzazioni precedenti
<span class="p">-</span> Crea nuove categorie solo quando strettamente necessario
<span class="p">-</span> Usa sempre singolare per le categorie (es. "Calcio" non "Calcistica")
<span class="p">-</span> Usa nomi semplici e diretti (es. "Politica" non "Scienze Politiche")

OUTPUT:
Devi sempre rispondere utilizzando esclusivamente questo formato XML:
<span class="nt">&lt;classe</span><span class="err">="</span><span class="na">CATEGORIA</span><span class="err">"</span> <span class="nt">/&gt;</span>

Dove CATEGORIA è la categoria tematica identificata.

ESEMPI DI CATEGORIZZAZIONE:
<span class="p">
-</span> Testi su malattie, cure, farmaci → "Medicina"
<span class="p">-</span> Testi su partite, campionati → "Calcio"
<span class="p">-</span> Testi su prodotti in vendita → "Pubblicità"
<span class="p">-</span> Testi su smartphone, computer → "Tecnologia"
<span class="p">-</span> Testi su ricette, cucina → "Gastronomia"
<span class="p">-</span> Testi pubblicitari in cui singole persone promuovono il proprio lavoro-&gt; "Autopromozione"

{% if examples%}

<span class="gu">## Esempi</span>

{% for ex in examples%}

<span class="gu">### Esempio {{loop.index}}</span>

Testo: {{ex.content}}
{% if ex.meta['quality']%}Qualità: {{ex.meta['quality']}}
{%endif%}{% if ex.meta['keywords']%}Parole Chiave: {{ex.meta['keywords']}}
{%endif%}Categoria: <span class="nt">&lt;classe</span><span class="err">="{{</span><span class="na">ex.meta</span><span class="err">['</span><span class="na">category</span><span class="err">']}}"</span> <span class="nt">/&gt;</span>
<span class="p">
---
</span>
{% endfor %}
{%endif%}
Categorie Esistenti:
{% for cat in categories%}
<span class="p">
-</span> "{{cat}}"
  {% endfor %}
  NOTA IMPORTANTE:
  Prima di creare una nuova categoria, verifica sempre se è possibile utilizzare una categoria esistente nell'elenco fornito. La creazione di nuove categorie deve essere l'ultima risorsa quando nessuna categoria esistente è appropriata.
</code></pre></div></div> </details> <details><summary>Prompt dell’utente - Template Jinja</summary> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gu">## Campione:</span>

Testo: {{campione.content}}
{% if campione.meta['quality']%}Qualità: {{campione.meta['quality']}}
{%endif%}{% if campione.meta['keywords']%}Parole Chiave: {{campione.meta['keywords']}}
{%endif%}
</code></pre></div></div> </details> <h2 id="visualizzazione">Visualizzazione</h2> <div class="l-page"> <iframe src="/assets/plotly/class_distribution.html" frameborder="0" scrolling="no" height="610" width="810" style="border: 1px dashed grey;"></iframe> </div> <p>Nel plot qua sopra sono riportati i 23 topic più frequenti scelti dall’LLM. In generale il dataset contiene argomenti abbastanza variegati. E’ interessante il grande numero di campioni marchiati con “Politica” (98). Questo è probabilmente dovuto al fatto che gran parte del testo consiste in articoli di giornale. Già in questo plot si possono notare parole chiave che indicano in realtà la stessa categoria, come “Cronaca” e “Notizie”. In una futura iterazione potrebbe aver senso fissare dei topic e impedire al modello di generare topic aggiuntivi, così che possa essere catturata una migliore distribuzione.</p> <div class="l-page"> <iframe src="/assets/plotly/sample_scatter.html" frameborder="0" scrolling="no" height="610" width="810" style="border: 1px dashed grey;"></iframe> </div> <p>Questo scatter-plot mostra la distribuzione dei campioni rispetto ai topic a cui sono stati appaiati e rispetto alla qualità del loro contenuto informativo. Il plot è interattivo, e si possono considerare i campioni di ogni qualità o anche un subset (magari si è interessati alla distribuzione di solo quelli con contenuto <strong>Excellent</strong>).</p> <p>La posizione del testo all’interno del plot è calcolata utilizzando un BERT italiano. Questi modelli sono in grado di ottenere in input del testo e tradurli in un vettore di numeri, e sono utilizzati anche per modellare la similarità tra testi diversi. In generale, campioni con posizioni vicine hanno contenuto simile. Per questo grafico in particolare, la posizione del campione dipende sia dal suo contenuto che dal suo topic. In particolare, il vettore della visualizzazione è una combinazione convessa del vettore del contenuto e del vettore del topic. Giocando un po’ con i filtri si può notare che purtroppo la distribuzione della qualità non è migliore o peggiore attorno ad alcuni particolari topic. Al contrario, tutti i topic sembrano avere grossolanamente lo stesso rapporto di campioni di cattiva e buona qualità.</p> <p>Ci sono 3 motivi per cui ho scelto di considerare nella visualizzazione non solo il contenuto, ma anche il topic.</p> <p>Il primo è che per qualche motivo nel mio ambiente di sviluppo <a href="https://distill.pub/2016/misread-tsne/" rel="external nofollow noopener" target="_blank">t-SNE</a> crasha ogni volta che provo ad utilizzarlo. Questo algoritmo in generale dà delle buone visualizzazioni per testo e immagini, in quanto cattura relazioni non lineari tra i campioni. Per questo, ho ripiegato sulla PCA, ma purtroppo le prima due dimensioni spiegano solo il 14% della varianza e il plot risultava essere molto confuso. Considerare anche il topic mi permette di avere una visualizzazione più chiara e raggruppata attorno a dei “punti topici”, mischiando il significato originale con quello del topic. Per mischiarli uso una combinazione convessa in cui mantengo il 40% degli embedding del contesto, assegnando il 60% ai topic.</p> <p>Il secondo motivo è che non ho un BERT allenato specificatamente su questi dati e su questo task di estrazione topic. Di conseguenza le rappresentazioni non sono particolarmente ottimali, ma comunque comprendono le relazioni semantiche tra i diversi campioni.</p> <p>Il terzo motivo è molto semplice: la visualizzazione risulta essere molto chiara e utile e mi ha permesso di trarre queste considerazioni.</p> <h2 id="conclusioni">Conclusioni</h2> <p>Alla fine le mie preoccupazioni di uno sbilanciamento su <em>Teologia</em> e <em>Marxismo</em> si sono rivelate infondate, quindi possiamo stare certi che gli LLM allenati su questi dati saranno di ottima qualità (si scherza…).</p> <p>Sono rimasto stupito dalla quantità di campioni segnati come <strong>“Good”</strong>, e pescandone qualcuno a caso non penso di essere d’accordo con alcuni annotatori. Se abbastanza persone annotano multiple volte lo stesso campione si potrebbero calcolare delle metriche di <em>agreement</em> tra i diversi annotatori e trovare campioni controversi.</p> <p>Sono rimasto un po’ dispiaciuto dal fatto che la qualità è divisa abbastanza omogeneamente in tutto il grafico. Questo potrebbe implicare che un modello che predice la qualità avrà bisogno di ben più di 1000 campioni. Oppure, più probabilmente, implica che una visualizzazione su solo due dimensioni non riesce a catturare bene la differenza di qualità.</p> <p>Spero in futuro di allenare questo classificatore, anche se devo dire che questa analisi mi ha lasciato con la pancia piena.</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fineweb-480.webp 480w,/assets/img/publication_preview/fineweb-800.webp 800w,/assets/img/publication_preview/fineweb-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/fineweb.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fineweb.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="fineweb" class="col-sm-8"> <div class="title">The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale</div> <div class="author"> Guilherme Penedo, Hynek Kydlı́ček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf </div> <div class="periodical"> <em>In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/root-prima-sessione/">Root, Incendi, Guerre Civili e Crimini di Guerra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/se-non-siamo-d-accordo/">Anche se non siamo d'accordo, ti do il permesso di esistere</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/silksong-la-vedova/">La Vedova è una bella bossfight</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/vibe-coding-100x-engineers/">Vibe Coding &amp; 100x Engineers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/silksong-prime-impressioni/">Silksong - Prime Impressioni</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Oscar Pindaro. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3b634bacfb790529570cd852feae2f80"></script> <script defer src="/assets/js/bootstrap-toc.min.js?b09662effa0739abf36e63d5ddf8979a"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?5e9d3cf00f58c3827975340a397d420c"></script> <script defer src="/assets/js/common.js?84ae21ea62fb6a7d7639c05a1d9fd8a0"></script> <script defer src="/assets/js/copy_code.js?a4a306698679d9f4baf589e11ed8d37c" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?a669d239c8a7e8916316237a1bed3ad7"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?ff39e530610476c24522cd1be2d56856"></script> <script src="/assets/js/vanilla-back-to-top.min.js?eaf77346e117baa09987a278a117b9a7"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?8d7ebe8276cfa922ec1506a5c6b20c13"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-progetti",title:"Progetti",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-root-incendi-guerre-civili-e-crimini-di-guerra",title:"Root, Incendi, Guerre Civili e Crimini di Guerra",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/root-prima-sessione/"}},{id:"post-anche-se-non-siamo-d-39-accordo-ti-do-il-permesso-di-esistere",title:"Anche se non siamo d&#39;accordo, ti do il permesso di esistere",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/se-non-siamo-d-accordo/"}},{id:"post-la-vedova-\xe8-una-bella-bossfight",title:"La Vedova \xe8 una bella bossfight",description:"Analisi del pattern di gioco della Vedova, per capire come mai \xe8 una boss fight cos\xec gustosa",section:"Posts",handler:()=>{window.location.href="/blog/2025/silksong-la-vedova/"}},{id:"post-vibe-coding-amp-100x-engineers",title:"Vibe Coding &amp; 100x Engineers",description:"Un argomento che mi porta molto vicino al Luddismo e allo Jihad Butleriano",section:"Posts",handler:()=>{window.location.href="/blog/2025/vibe-coding-100x-engineers/"}},{id:"post-silksong-prime-impressioni",title:"Silksong - Prime Impressioni",description:"Prime impressioni dopo 5 ore di gioco.",section:"Posts",handler:()=>{window.location.href="/blog/2025/silksong-prime-impressioni/"}},{id:"post-fineweb-community-analisi-e-estrazione-topic",title:"Fineweb-Community - Analisi e Estrazione Topic",description:"Un framework per convertire personaggi in Language Models",section:"Posts",handler:()=>{window.location.href="/blog/2025/fineweb-community-italian/"}},{id:"post-lemonpc",title:"LeMoNPC",description:"Un framework per convertire personaggi in Language Models",section:"Posts",handler:()=>{window.location.href="/blog/2024/roleplay-finetuning/"}},{id:"post-imparando-manim",title:"Imparando Manim",description:"Una raccolta di animazioni per imparare ad usare meglio manim",section:"Posts",handler:()=>{window.location.href="/blog/2024/esperimenti-manim/"}},{id:"post-algoritmi-genetici-e-le-8-regine",title:"Algoritmi Genetici e le 8 regine",description:"Modellare utilizzando algoritmi genetici, applicato al problema delle 8 regine",section:"Posts",handler:()=>{window.location.href="/blog/2024/algoritmi-genetici/"}},{id:"news-il-mio-blog-\xe8-finalmente-online",title:"Il mio blog \xe8 finalmente online!",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6F%73%63%61%72.%70%69%6E%64%61%72%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/OscarPindaro","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/oscar-pindaro","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>