<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="it"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://oscarpindaro.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://oscarpindaro.github.io/" rel="alternate" type="text/html" hreflang="it"/><updated>2025-04-12T19:40:20+00:00</updated><id>https://oscarpindaro.github.io/feed.xml</id><title type="html">blank</title><subtitle>Questo blog è il mio spazio dedicato per approfondire e condividere le mie esperienze nel mondo dell&apos;Intelligenza Artificiale e del Game Design. Nasce con l&apos;obiettivo di consolidare i concetti appresi durante il mio lavoro con l&apos;IA e di narrare la mia avventura nel game design e nello sviluppo con il gruppo Wauhaus. È un punto d&apos;incontro per esplorare nuove idee, discutere le sfide e condividere le migliori pratiche in entrambi i campi. </subtitle><entry><title type="html">Fineweb-community topic distribution</title><link href="https://oscarpindaro.github.io/blog/2025/fineweb-community-italian/" rel="alternate" type="text/html" title="Fineweb-community topic distribution"/><published>2025-02-23T18:00:00+00:00</published><updated>2025-02-23T18:00:00+00:00</updated><id>https://oscarpindaro.github.io/blog/2025/fineweb-community-italian</id><content type="html" xml:base="https://oscarpindaro.github.io/blog/2025/fineweb-community-italian/"><![CDATA[<p>La tabella qua sotto riporta il numero di dataset e modelli NLP disponibili attualmente su <a href="https://huggingface.co/">HuggingFace</a>. Ovviamente, la lingua inglese regna su tutti, con più di 15 mila dataset e 150 mila modelli allenati su diversi domini (generale, biomedico, legale, etc). La lingua italiana vede invece una rappresentazione molto più magra, sotto tutte le altre principali lingue europee.</p> <table> <thead> <tr> <th style="text-align: left">Lingua</th> <th style="text-align: center">Dataset</th> <th style="text-align: center">Modelli</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Italiano</td> <td style="text-align: center">660</td> <td style="text-align: center">6,254</td> </tr> <tr> <td style="text-align: left">Inglese</td> <td style="text-align: center"><strong>16,746</strong></td> <td style="text-align: center"><strong>154,599</strong></td> </tr> <tr> <td style="text-align: left">Francese</td> <td style="text-align: center">1,404</td> <td style="text-align: center">8,951</td> </tr> <tr> <td style="text-align: left">Spagnolo</td> <td style="text-align: center">1,167</td> <td style="text-align: center">8,092</td> </tr> <tr> <td style="text-align: left">Tedesco</td> <td style="text-align: center">943</td> <td style="text-align: center">8,004</td> </tr> </tbody> </table> <p>E’ chiaro che l’inglese è la lingua degli LLM, mentre le altre lingue sono molto più marginali. E questo è ancora più lampante quando si interagisce con un qualsiasi modello. I grandi modelli privati (come ChatGPT e Claude) conversano abbastanza agilmente in italiano, mentre quelli open-source sono ancora abbastanza acerbi. In particolare, manca ancora una buona alternativa computazionalmente efficiente che può conversare in italiano su hardware modesti.</p> <p>La penuria di dataset open italiani rende lo sviluppo di migliori modelli poco conveniente e razionale, e le BigTech come Meta, Google e OpenAI preferiscono ottimizzare innanzitutto sulla lingua più parlata in occidente.</p> <h2 id="fineweb-edu">Fineweb-Edu</h2> <p>Nel maggio 2024 HuggingFace ha rilasciato Fineweb <a class="citation" href="#fineweb">(Penedo et al., 2024)</a>, un dataset di 15 trilioni (!!!) di token in lingua inglese. Il dataset è stato creato a partire da <a href="https://commoncrawl.org/">CommonCrawl</a>, un dump di tutti i contenuti presenti su internet fino a quel momento. Questo dataset contiene il codice sorgente html dei siti, e quindi HuggingFace ha dovuto innanzitutto trasformarlo in un formato leggibile da LLM e esseri umani.</p> <p>Tuttavia, non basta avere accesso a questi dati. Internet infatti è pieno di siti web spam, di bassa qualità, generati automaticamente, e così via. Nel report è riportato dettagliatamente come HuggingFace ha ripulito il dataset originario, concentrandosi su due aspetti principali:</p> <ul> <li><strong>Deduplicazione</strong>: è un processo che si occupa di scartare contenuto ridondante. Questo viene fatto per evitare che contenuti presenti in molte copie monopolizzino i parametri dei modelli che verranno allenati</li> <li><strong>Qualità</strong>: parte del contenuto risulta avere problemi di formattazione, testo generato automaticamente da bot, miscugli di lingue incoerenti. Questi campioni devono essere buttati visto che avrebbero su qualsiasi training un contributo estremamente negativo.</li> </ul> <p>Insieme a questo dataset, è stata anche pubblicata una sotto-porzione di 1.3 Trilioni di token chiamata <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">Fineweb-Edu</a>. Questa dataset ha un focus su contenuti educativi, come blogpost, paper, lezioni universitarie.</p> <p>Per poter estratte questi campioni di alta qualità, HuggingFace ha annotato un subset di Fineweb con Llama3-70B-Instruct, dividendo i campioni per contenuto e qualità informativa. Da queste annotazioni ha costruito poi un classificatore molto più snello e rapido che si occupa di classificare su larga scala il resto del dataset.</p> <p>La lingua principale di Llama è l’inglese, e quindi questo filtraggio non funziona molto bene su campioni in altre lingue, in quanto le performance di classificazione sarebbero molto più basse.</p> <p>Per superare questa limitazione dei modelli generativi attuali, è stato quindi creato Fineweb-C.</p> <h2 id="fineweb-c">Fineweb-C</h2> <p><a href="https://huggingface.co/datasets/data-is-better-together/fineweb-c">Fineweb-C</a> può essere considerato il naturale passo successivo a FineWeb, estendendo l’analisi fatta in precedenza su tutte le restanti lingue. L’idea di base è sempre la stessa: trovare un modo per distinguere campioni di bassa qualità da campioni informativi. Se in FineWeb si usava un LLM, per questo dataset invece HuggingFace ha chiesto aiuto alla community. Per ogni lingua sono stati estratti 1000 campioni, che sono stati e stanno venendo annotati da dei volontari fluenti in quella lingua.</p> <p>In particolare, gli annotatori devono classificare i campioni nelle seguenti classi:</p> <ul> <li><strong>None</strong>: il campione non ha alcun contenuto informativo. Può essere pubblicità, spam, un post su un social che non parla di niente di particolare, news di gossip</li> <li><strong>Minimal</strong>: il campione contiene un contenuto informativo molto minimo, come ad esempio un articolo di giornale che parla di un particolare evento senza dilungarsi troppo</li> <li><strong>Basic</strong>: il campione ha un principio di intento educativo, come una definizione, una breve spiegazione</li> <li><strong>Good</strong>: il testo è una spiegazione abbastanza dettagliata, anche ben formattata. La maggior parte dei concetti sono esposti chiaramente</li> <li><strong>Excellent</strong>: il testo è di qualità sopraffina, molto ben formattato. Un esempio potrebbe essere un blogpost molto tecnico oppure una dispensa universitaria</li> <li><strong>Problematic Content</strong>: questa è una categoria ombrello in cui finiscono tutti quei test formattati male, oppure spam, pornografia, materiale sensibile, etc.</li> </ul> <p>Lo split italiano è stato, ad oggi, annotato da 26 annotatori. Io personalmente ho contribuito annotando circa 400 campioni. Questi 1000 campioni sono stati pescati casualmente dal dataset originale, pre-filtrando materiale problematico (come pornografia, gioco d’azzardo, etc). Devo dire che la pipeline di pre-filtraggio ha funzionato molto bene, visto che non mi è capitato quasi mai di trovare campioni imbarazzanti. Ho letto che non è stato così semplice in altri casi, specialmente per le lingue del sud-est asiatico, in cui la pipeline ha fatto passare moltissima pornografia.</p> <h3 id="perchè-è-importante">Perchè è importante</h3> <p>Come già detto prima, l’Italiano è una lingua molto poco rappresentata. Quando cerco modelli per il mio lavoro di tutti i giorni, sono sempre costretto a scegliere LLM generici allenati su dataset che iniziano a mostrare i segni del tempo. Provo una grande invidia per chi sviluppa in inglese, che ha addirittura classificatori e retriever già pronti per il campo medico o altri campi estremamente specifici.</p> <h3 id="criticità">Criticità</h3> <p>Mi sembra onesto parlare anche dei punti un po’ più dolorosi della questione. Mentre annotavo il dataset, ho avuto il sospetto di star leggendo materiale coperto da copyright. La proprietà intellettuale è sempre un argomento caldo su cui c’è molta ipocrisia. Ci si interessa solo se si è vittime del fenomeno, mentre è facile ignorarlo se non si è un creatore di contenuti. Questo è ovviamente un tema molto ampio che inizia da ben prima dell’avvento dell’AI generativa, ma secondo me non è mai stato così attuale.</p> <p>Probabilmente cambierò idea altre 100 volte su questo. Attualmente penso che la natura open di questo progetto lo allevia un po’ dalle invevitabili colpe di cui si macchierà quando verrà scalato sull’intero dataset. Spero però che parlandone il grande pubblico possa iniziare a capire un po’ meglio l’origine di questi dati e inizi una conversazione attorno a questo argomento.</p> <p>C’è poi ovviamente tutto un tema secondario sul fatto che i modelli generativi vengono utilizzati molto per generare fake news e scam. Di sicuro rendere gli LLM fluenti in multiple lingue aumenterà il raggio di questa piaga, ma almeno per design questo dataset prova a raccogliere campioni con contenuto educativo.</p> <h2 id="analisi-del-dataset">Analisi del dataset</h2> <p>Mentre annotavo il dataset, mi è sembrato che i campioni di altà qualità fossero soprattuto in ambito teologico e poltico. Ho infatti trovato molti testi estratti da del catechismo e da riflessioni sul capitale di Marx.</p> <p>Per questo motivo, ho deciso di fare un’analisi del dataset per vedere la distribuzione dei topic. L’analisi avverò in questa maniera:</p> <ol> <li>estrazioni di parole chiave dal testo</li> <li>utilizzo di un LLM per estrarre un topic generali</li> <li>visualizzazione dei dati e considerazioni</li> </ol> <p>Il codice sorgente lo trovate qua <a href="https://github.com/OscarPindaro/fineweb-c-analysis-ita">nella mia repo.</a>.</p> <h2 id="estrazione-delle-parole-chiave">Estrazione delle parole chiave</h2> <p>Visto che ho intenzione di usare un LLM per estrarre dei topic di alto livello dal testo (come ad esempio “Moda”, “Tecnologia”, “Teologia”), ho deciso innanzitutto di estrarre delle parole chiave per ogni campione del dataset. Le parole chiave sono presenti all’interno dei campioni, e quindi io le considero di <em>basso livello</em> in quanto sono spesso molto specifiche per il testo analizzato. Questo passo è fondamentale perchè mi permette:</p> <ul> <li>di conoscere meglio i contenuti del dataset</li> <li>di capire che analisi devo fare</li> <li>di dare informazioni aggiuntive all’LLM per aiutarlo nel suo lavoro Purtroppo non sono riuscito a fare nessuno studio di ablazione, ma mi sembra una intuizione ragionevole e comunque queste parole chiave le avrei estratte in ogni caso.</li> </ul> <p>Per trovare queste parole chiave ho usato <strong>TF-IDF</strong> (Term Frequency - Inverse Document Frequency). L’intuizione dietro a questo algoritmo è la seguente: una parola non è importante se è in termini assoluti è molto o poco presente, ma è importante se è presente solo in questo particolare campione e assente negli altri. Questo permette scovare per ogni documento le parole che lo identificano unicamente, considerando la loro frequenza assoluta ma anche la frequenza all’interno del singolo testo. Ad esempio, se ho due documenti diversi, uno che è un articolo che descrive la vita di uno scienziato, e un altro che descrive la vita dello scienziato Enrico Fermi, per entrambi la parola <em>scienziato</em> è molto importante, ma per il secondo le parole Enrico Fermi lo rendono unico.</p> <p>Ecco degli esempi di parole chiave che ho estratto:</p> <ul> <li>dal campione 1 ho estratto le parole chiave <code class="language-plaintext highlighter-rouge">['fedi', 'oro', 'anello', 'fondere', 'anelli']</code>, quindi probabilmente parlerà di <em>Matrimonio</em>, o comunque <em>Religione</em></li> <li>dal campione 12 ho estratto le parole chiave <code class="language-plaintext highlighter-rouge">['plusvalore', 'capitale', 'produzione', 'merce', 'accumulazione']</code>, quindi probabilmente parlerà di <em>Economia</em> (o di <em>Marxismo</em>)</li> </ul> <p>Per questo algoritmo è molto importante fare un po’ di data cleaning, rimuovendo articoli, preposizioni, numeri e altre parole ad alta frequenza ma poco interessanti.</p> <h2 id="estrazione-dei-topic-di-alto-livello">Estrazione dei Topic di alto livello</h2> <p>Una volta estratte le keyword, bisogna estrarre i topic generali. Per questi, ho deciso di utilizzare Llama.1 quantizzato a 8 bit (<code class="language-plaintext highlighter-rouge">llama3.1:8b-instruct-q8_0</code>) e Gemma 2 (<code class="language-plaintext highlighter-rouge">gemma2:2b</code>), visto che riesco a lanciarli sulla mia <strong>RTX 4070 12GB</strong>. Ho anche provato a utilizzare una versione distillata di deepseek (<code class="language-plaintext highlighter-rouge">llama3.1:8b-instruct-q8_0</code>), ma come mostrerò più avanti i tempi di calcolo erano un po’ troppo lunghi e ho preferito ignorarlo. Il prompt di sistema è strutturato nella seguente maniera:</p> <ul> <li>spiegazione delle classi di qualità di Fineweb-C</li> <li>Indicazioni su che informazioni il modello ha accesso (elenco di parole chiave, testo).</li> <li>Regole di categorizzazione: il modello deve dare categorie di alto livello e non specifiche. Ha accesso ad una lista di categorie pre-calcolate, ma può comunque scegliere di assegnare una nuova categoria non esistente.</li> <li>Formato dell’output: il modello deve scrivere tutto all’interno di tag xml, visto che sono semplici da parsare.</li> </ul> <p>Il modello ha la libertà di scegliere una classe anche se non è presente tra quelle fornite. Questo mi permette di avere un po’ di flessibilità, anche perchè è un dataset che conosco poco.<br/> Operativamente, quando il modello sceglie una classe non presente tra quelle esistenti, viene aggiunta alla lista delle classi possibili, condizionando le future generazioni.</p> <p>Alla fine di questo processo, Llama ha estratto circa 142 topic. Ci sono alcuni casi in cui alcune categorie sono duplicate (singolare/plurale) ma per la visualizzazione di seguito non avrà molto peso.</p> <p>Gemma invece ha estratto 94 categorie, ma ho preferito quelle estratte da Llama</p> <p>Per servire Llama e Gemma ho usato <a href="https://ollama.com/">ollama</a>. L’ultima versione di ollama ha introdotto il <code class="language-plaintext highlighter-rouge">prompt caching</code>, che permette di mantenere una cache delle ultime richieste fatte al modello. Nel caso di prompt molto simili tra una sessione all’altra, questa cache viene utilizzata per evitare di ricalcolare tutti i valori dell’attenzione del prompt di sistema, aumentando drammaticamente la velocità di generazione. Questo mi ha permesso di avere i topic generali in una decina di minuti. Purtroppo con DeepSeek questo vantaggio non si è presentato: poiché è un modello “reasoning”, prima di dare una risposta, emette molti token di ragionamento, e il tempo di annotazione quindi è aumentato da 10 minuti a 4 ore.</p> <p>Ancora una volta sono rimasto molto stupito dalle performance di Llama, mentre gemma non mi ha reso particolarmente entusiasta.</p> <details><summary>Prompt di sistema - Tempalte Jinja</summary> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sei un assistente specializzato nell'analisi e classificazione di testi in italiano. Il tuo compito è duplice:
<span class="p">
1.</span> Comprendere il livello qualitativo del contenuto informativo del testo, basandoti sulla seguente scala:
<span class="p">   -</span> Problematic Content: contenuti inappropriati (pornografia, gambling, testo mal formattato)
<span class="p">   -</span> None: assenza di contenuto informativo (es. pubblicità, post social)
<span class="p">   -</span> Minimal: contenuto con minima valenza informativa non intenzionale
<span class="p">   -</span> Basic: contenuto con discreto valore informativo
<span class="p">   -</span> Good: contenuto ben strutturato con chiaro intento educativo
<span class="p">   -</span> Basic: contenuto con elevato valore informativo e ottima strutturazione
<span class="p">
2.</span> Identificare una categoria tematica di alto livello che rappresenti l'argomento principale del testo.

CONTESTO OPERATIVO:
<span class="p">-</span> Hai accesso a un elenco di parole chiave di basso livello estratte dal testo
<span class="p">-</span> Hai accesso a un elenco di categorie tematiche già utilizzate in precedenza
<span class="p">-</span> Puoi sia utilizzare categorie esistenti che crearne di nuove quando necessario

REGOLE DI CATEGORIZZAZIONE:
<span class="p">-</span> Usa categorie ampie e generali (es. "Medicina", "Sport", "Tecnologia")
<span class="p">-</span> Mantieni consistenza con le categorizzazioni precedenti
<span class="p">-</span> Crea nuove categorie solo quando strettamente necessario
<span class="p">-</span> Usa sempre singolare per le categorie (es. "Calcio" non "Calcistica")
<span class="p">-</span> Usa nomi semplici e diretti (es. "Politica" non "Scienze Politiche")

OUTPUT:
Devi sempre rispondere utilizzando esclusivamente questo formato XML:
<span class="nt">&lt;classe</span><span class="err">="</span><span class="na">CATEGORIA</span><span class="err">"</span> <span class="nt">/&gt;</span>

Dove CATEGORIA è la categoria tematica identificata.

ESEMPI DI CATEGORIZZAZIONE:
<span class="p">-</span> Testi su malattie, cure, farmaci → "Medicina"
<span class="p">-</span> Testi su partite, campionati → "Calcio"
<span class="p">-</span> Testi su prodotti in vendita → "Pubblicità"
<span class="p">-</span> Testi su smartphone, computer → "Tecnologia"
<span class="p">-</span> Testi su ricette, cucina → "Gastronomia"
<span class="p">-</span> Testi pubblicitari in cui singole persone promuovono il proprio lavoro-&gt; "Autopromozione"

{% if examples%}
<span class="gu">## Esempi</span>
{% for ex in examples%}
<span class="gu">### Esempio {{loop.index}}</span>
Testo: {{ex.content}}
{% if ex.meta['quality']%}Qualità: {{ex.meta['quality']}}
{%endif%}{% if ex.meta['keywords']%}Parole Chiave: {{ex.meta['keywords']}}
{%endif%}Categoria: <span class="nt">&lt;classe</span><span class="err">="{{</span><span class="na">ex.meta</span><span class="err">['</span><span class="na">category</span><span class="err">']}}"</span> <span class="nt">/&gt;</span>
<span class="p">
---</span>
{% endfor %}
{%endif%}
Categorie Esistenti:
{% for cat in categories%}
<span class="p">-</span> "{{cat}}"
{% endfor %}
NOTA IMPORTANTE:
Prima di creare una nuova categoria, verifica sempre se è possibile utilizzare una categoria esistente nell'elenco fornito. La creazione di nuove categorie deve essere l'ultima risorsa quando nessuna categoria esistente è appropriata.
</code></pre></div></div> </details> <details><summary>Prompt dell’utente - Tempalte Jinja</summary> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">Campione:
---
</span>Testo: {{campione.content}}
{% if campione.meta['quality']%}Qualità: {{campione.meta['quality']}}
{%endif%}{% if campione.meta['keywords']%}Parole Chiave: {{campione.meta['keywords']}}
{%endif%}
</code></pre></div></div> </details> <h2 id="visualizzazione">Visualizzazione</h2> <div class="l-page"> <iframe src="/assets/plotly/class_distribution.html" frameborder="0" scrolling="no" height="610" width="810" style="border: 1px dashed grey;"></iframe> </div> <p>Nel plot qua sopra sono riportati i 23 topic più frequenti scelti dall’LLM. In generale il dataset contiene argomenti abbastanza variagati. E’ interessante il grande numero di campioni marchiati con “Politica” (98). Questo è probabilmente dovuto al fatto che gran parte dei campioni sono articoli o piccoli estratti di articoli. Già in questo plot si possono notare parole chiave che indicano in realtà la stessa categoria, come “Cronaca” e “Notizie”. In una futura iterazione potrebbe aver senso fissare dei topic e impedire al modello di generarne topic aggiuntivi, così che possa essere catturata una migliore distribuzione.</p> <div class="l-page"> <iframe src="/assets/plotly/sample_scatter.html" frameborder="0" scrolling="no" height="610" width="810" style="border: 1px dashed grey;"></iframe> </div> <p>Questo scatter-plot mostra la distribuzione dei campioni rispetto ai topic a cui sono stati appaiati e rispetto alla qualità del loro contenuto informativo. Il plot è interattivo, e si possono considerare i campioni di ogni qualità o anche un subset (magari si è interessati alla distribuzione di solo quelli con contenuto <strong>Excellent</strong>).</p> <p>La posizione del testo all’interno del plot è calcolata utilizzando un BERT italiano. Questi modelli sono in grado di ottenere in input del testo e tradurli in un vettore di numeri, e sono utilizzati anche per modellare la similarità tra testi diversi. In generale, campioni con posizioni vicine hanno contenuto simile. Per questo grafico in particolare, la posizione del campione dipende sia dal suo contenuto che dal suo topic. In particolare, il vettore della visualizzazione è una combinazione convessa del vettore del contenuto e del vettore del topic. Giocando un po’ con i filtri si può notare che purtroppo la distribuzione della qualità non è migliore o peggiore attorno ad alcuni particolari topic. Al contrario, tutti i topic sembrano avere grossolanamente lo stesso rapporto di campioni di cattiva e buona qualità.</p> <p>Ci sono 3 motivi per cui ho scelto di considerare nella visualizzazione non solo il contenuto, ma anche il topic.</p> <p>Il primo è che per qualche motivo nel mio ambiente di sviluppo <a href="https://distill.pub/2016/misread-tsne/">t-SNE</a> crasha ogni volta che provo ad utilizzarlo. Questo algoritmo in generale dà delle buone visualizzazioni per testo e immagini, in quanto cattura relazioni non lineari tra i campioni. Per questo, ho ripiegato sulla PCA, ma purtroppo le prima due dimensioni spiegano solo il 14% della varianza e il plot risulta essere molto confuso. Considerare anche il topic mi permette di avere una visualizzazione più chiara e raggruppata attorno a dei “punti topici”, mischiando il significato originale con quello del topic. Per mischiarli uso una combinazione convessa in cui mantengo il 40% degli embedding del contesto, assegnando il 60% ai topic.</p> <p>Il secondo motivo è che non ho un BERT allenato specificatamente su questi dati e su questo task di estrazione topic. Di conseguenza le rappresentazioni non sono particolarmente ottimali, ma comunque comprendono le relazioni semantiche tra i diversi campioni.</p> <p>Il terzo motivo è molto semplice: la visualizzazione risulta essere molto chiara e utile e mi ha permesso di trarre queste considerazioni.</p> <h2 id="conclusioni">Conclusioni</h2> <p>Alla fine le mie preoccupazioni di uno sbilanciamento su <em>Teologia</em> e <em>Marxismo</em> si sono rivelate infondate, quindi possiamo stare certi che gli LLM allenati su questi dati saranno di ottima qualità (si scherza…). Sono rimasto stupito dalla quantità di campioni segnati come <strong>“Good”</strong>, e pescandone qualcuno a caso non penso di essere d’accordo con alcuni annotatori. Se abbastanza persone annotano multiple volte lo stesso campione si potrebbero calcolare delle metriche di <em>agreement</em> tra i diversi annotatori e trovare campioni controversi.</p> <p>Sono rimasto un po’ dispiaciuto dal fatto che la qualità è divisa abbastanza omogeneamente in tutto il grafico. Questo potrebbe implicare che un modello che predice la qualità avrà bisogno di ben più di 1000 campioni. Oppure, più probabilmente, implica che una visualizzazione su solo due dimensioni non riesce a catturare bene la differenza di qualità.</p> <p>Spero in futuro di allenare questo classificatore, anche se devo dire che questa analisi mi ha lasciato con la pancia piena.</p>]]></content><author><name></name></author><category term="llm"/><category term="transformers"/><category term="genenerative-ai"/><category term="llm"/><category term="ai"/><category term="transformers"/><category term="gen-ai"/><summary type="html"><![CDATA[Un framework per convertire personaggi in Language Models]]></summary></entry><entry><title type="html">LeMoNPC</title><link href="https://oscarpindaro.github.io/blog/2024/roleplay-finetuning/" rel="alternate" type="text/html" title="LeMoNPC"/><published>2024-12-27T18:00:00+00:00</published><updated>2024-12-27T18:00:00+00:00</updated><id>https://oscarpindaro.github.io/blog/2024/roleplay-finetuning</id><content type="html" xml:base="https://oscarpindaro.github.io/blog/2024/roleplay-finetuning/"><![CDATA[<h2 id="cosè-lemonpc">Cos’è LeMoNPC?</h2> <p>LeMoNPC (<strong>L</strong>anguage <strong>M</strong>odel <strong>N</strong>on-<strong>P</strong>layable <strong>C</strong>haracter) è un progetto che si pone come obiettivo la creazione automatica di NPC (Non-Playable Character), personaggi che all’interno di un videogioco comunicano con il giocatore.</p> <p>Un NPC ha un ruolo più o meno importante all’interno della struttura del gioco, in quanto può:</p> <ul> <li>spiegare e portare avanti la trama principale del gioco</li> <li>spostare l’attenzione del giocatore su una sotto-trama secondaria</li> <li>dare missioni al giocatore</li> </ul> <p>L’obiettivo di questo framework è quello di dare a scrittori di storie e personaggi un tool per rendere “vivi” i loro personaggi. <strong>LeMoNPC</strong> si occuperà di <strong>raccogliere le informazioni</strong> su loro, <strong>finetunare un LLM</strong> (Large Language Model) per imparare a rispondere come loro, <strong>gestire le memorie</strong> del mondo di gioco e così via.</p> <p>Una volta pronto, questo LLM potrebbe essere usato per avere delle risposte personalizzate in tempo reale in una campagna di Dungeon&amp;Dragons. Oppure essere integrato all’interno di un videogioco, in cui gli utenti possono scrivere quello che vogliono ai personaggi. O ancora semplicemente avere un personaggio con cui chiacchierare e fare delle domande. Tutto questo seguendo la visione originale che il designer aveva per il personaggio. Questo può permettere di scrivere personaggi che non vivono su binari prefissati, ma che possono reagire a qualsiasi tipo di input.</p> <p>In questo post presenterò un primo prototipo di <strong>LeMoNPC</strong>, sviluppato in poco tempo come proof-of-concept e sfida personale. Per testare il framework, ho utilizzato dei Large Language Model per generare sia il personaggio (Orlando Marlo, un nobile del Regno di Luminaria) che un dataset di conversazioni che lo riguardano. L’obiettivo finale è quello di distillare queste informazioni, generate con modelli molto espressivi ma pesanti, in un modello molto più leggero e utilizzabile.</p> <p>In questo post vedremo:</p> <ul> <li>Le principali difficoltà tecniche</li> <li>Il processo di creazione del personaggio e del suo mondo</li> <li>La generazione del dataset di conversazioni del personaggio</li> <li>L’allenamento di LeMoNPC per replicare il comportamento del personaggio</li> </ul> <h2 id="cosa-non-è-lemonpc">Cosa non è LeMoNPC</h2> <p>Questo non è un framework per generare automaticamente personaggi. Non è un’interfaccia che crea un personaggio da un prompt generico. L’obiettivo è partire da un personaggio già creato. Deve avere una storia, un mondo originale, esempi di conversazioni, opinioni.</p> <p>Il focus di questo post è sulle tecniche per creare un NPC digitale, quindi ho utilizzato degli LLM per generare il mio personaggio base. Vedremo che in alcuni casi questo ha influito negativamente su alcuni risultati, e la mia supervisione è stata comunque necessaria.</p> <h2 id="perché-lemonpc">Perché LeMoNPC?</h2> <p>Nonostante ChatGPT sia stato rilasciato alla fine del 2022, non sono presenti sul mercato videogiochi mainstream che implementano questa tecnologia (se tralasciamo la pila di applicazioni di <em>fidanzate AI</em>). Questo è dovuto al fatto che questa tecnologia utilizza molte risorse computazionali ed economiche.</p> <p>Nel caso di LLM <strong>closed-source</strong> (gestiti a porte chiuse dalle aziende produttrici), l’utilizzo è pagato con una tariffa proporzionale alla quantità di testo generato. Utilizzare questi modelli obbligherebbe allo sviluppo di giochi live-service, con una monetizzazione molto aggressiva che serve a coprire i costi.</p> <p>Nel caso di modelli <strong>open-source</strong> (scaricabili e utilizzabili sulla propria macchina locale), il problema principale è legato alle loro dimensioni. L’hardware che li ospita deve:</p> <ul> <li>essere abbastanza potente per generare testo ad una velocità ragionevole;</li> <li>avere abbastanza memoria per ospitare il modello.</li> </ul> <p>La <a href="https://www.nvidia.com/it-it/geforce/10-series/">Nvidia RTX 1060</a> è probabilmente una delle schede video più comprate di sempre. Uscita nel 2016, è già un pezzo di hardware abbastanza vecchiotto, e comunque farebbe fatica a ospitare anche i modelli più piccoli: un modello può essere compresso in 4 GB di VRAM con molti sacrifici, e questa scheda è spesso venduta con 6 GB di VRAM. L’utilizzo di così tante risorse mette anche grandi limiti al tipo di gioco che può essere sviluppato, visto che il Language Model entra in competizione con la logica del gioco, la pipeline di rendering e eventuali intelligenze artificiali. Inoltre, non tutti i giocatori hanno a disposizione un computer con una scheda grafica.</p> <p>Un altro problema degli LLM (sia closed che open) è che presentano un <strong>moralismo</strong> corporate abbastanza spiccio, danneggiando la loro capacità di roleplay. La tragica conseguenza è una generale sensazione di cringe dell’utente (molto frequentemente questi modelli si comportano da bacchettoni) e un alto livello di <strong>refusal</strong>, ovvero la predisposizione a rifiutarsi a parlare di alcuni argomenti. Questo è un problema nel caso un NPC volesse anche banalmente parlare di alcol.</p> <p>Il mio sogno è quello di riuscire a digitalizzare un NPC <strong>usando tra i 200 MB e i 2GB di RAM</strong>, senza l’utilizzo di hardware specializzato come le schede grafiche. Questo permetterebbe anche a giocatori senza una macchina particolarmente potente di poter interagire con i personaggi.</p> <p>LeMoNPC deve essere in grado di produrre un LLM che si comporta come un personaggio scritto da un essere umano. L’essere umano può dare informazioni su questo personaggio, come backstory, esempi di conversazione, eventi importanti, valori, etc. Opzionalmente, si può fornire un dataset di conversazioni del personaggio, che può essere già utilizzato per allenare un piccolo LLM. Altrimenti, LeMoNPC si occuperà di generarlo.</p> <blockquote> <p><em>“If life gives you LeMoNs, create an NPC from your DnD character”</em></p> <p><strong>Socrates, probably</strong></p> </blockquote> <h2 id="creazione-del-personaggio">Creazione del Personaggio</h2> <p>Ovviamente il primo passaggio per utilizzare LeMoNPC è avere un personaggio. In questa sezione spiego come ho generato <strong>Orlando Marlo, Cavaliere di Luminaria</strong>.</p> <h3 id="contesto-storico">Contesto Storico</h3> <p>La creazione del contesto storico è fondamentale per definire il palcoscenico in cui i personaggi opereranno. Infatti, un personaggio non vive da solo nel vuoto, ma assume un certo ruolo in una società, ha delle opinioni, delle relazioni, e tanto altro.</p> <p>In questo prototipo, ho generato il contesto storico utilizzando Claude Sonnet. La generazione in sé non era male, anche se conteneva alcune frasi o espressioni che trovavo troppo corporate o generiche e che ho pulito. Ogni tanto aggiungevo io personalmente qualche dettaglio per migliorare un po’ la qualità della generazione.</p> <p>Il contesto storico si articola in diverse sezioni chiave:</p> <ul> <li><strong>Descrizione:</strong> Una breve introduzione che offre un primo sguardo al mondo, descrivendo le sue caratteristiche principali.</li> <li><strong>Organizzazione Politica:</strong> Definisce come il potere è strutturato e distribuito all’interno del contesto. Questa sezione definisce i meccanismi di governo e le dinamiche di potere che influenzeranno le scelte e le azioni dei personaggi.</li> <li><strong>Entità di Potere:</strong> Le organizzazioni o individui che detengono il controllo politico ed economico, influenzando la vita quotidiana degli abitanti del mondo.</li> <li><strong>Persone Importanti:</strong> Figure chiave della società, individuate per il loro ruolo sociale, culturale o politico. Queste figure possono offrire spunti interessanti per dialoghi e interazioni con il personaggio.</li> <li><strong>Entità Marginalizzate:</strong> Chi vive ai margini del sistema, spesso oppresso o svantaggiato rispetto ai gruppi dominanti. La loro presenza aggiunge profondità al mondo, evidenziando le disuguaglianze e le tensioni sociali e rendendo un po’ più tridimensionale l’ambientazione.</li> <li><strong>Persone Importanti Marginalizzate:</strong> Simili alle persone importanti, ma appartengono a gruppi che si oppongono al sistema dominante, in cerca di rivalsa contro un sistema che li ha scartati e li opprime.</li> </ul> <p>Nel mio caso, ho creato il <strong>Sacro Regno di Luminaria</strong>, una monarchia che condivide il potere con la Chiesa. La legittimità del regno dipende dalla percezione che i suoi sovrani siano stati scelti da Solaris, la divinità venerata nel regno. Il governo ha un sistema di doppio potere, dove le decisioni importanti richiedono l’approvazione sia della Corona che del Concilio Solare. Molti gruppi sono marginalizzati o oppressi, tra cui coloro che vengono considerati non favoriti da Solaris e i praticanti delle tradizioni pre-Solaris.</p> <h3 id="personaggio">Personaggio</h3> <p>Una volta creato il contesto, si può procedere con il personaggio, che ha:</p> <ul> <li><strong>Backstory:</strong> Una breve panoramica del passato del personaggio, contenente eventi importanti che hanno plasmato la sua personalità e le sue scelte.</li> <li><strong>Allineamento:</strong> <a href="https://dungeonedraghi.it/regole/personaggio/allineamento/">Un sistema semplificato per categorizzare il personaggio</a> in base alla sua posizione sociale, alle sue convinzioni morali e al suo atteggiamento nei confronti dell’autorità. Questo elemento aiuta a definire i suoi legami con gli altri personaggi e la società nel suo complesso.</li> <li><strong>Valori:</strong> Una lista di principi fondamentali che guidano le azioni e le decisioni del personaggio. Questi valori sono cruciali per creare un personaggio autentico e credibile.</li> <li><strong>Obiettivi:</strong> Cosa spinge il personaggio a agire? Quali sono i suoi desideri, le sue aspirazioni e i suoi sogni? I suoi obiettivi forniscono una direzione alla sua vita e lo spingono ad interagire con gli altri personaggi e con l’ambiente circostante.</li> <li><strong>Opinioni:</strong> Una serie di posizioni assunte dal personaggio su diversi argomenti. Le opinioni riflettono il suo punto di vista sul mondo, le sue convinzioni e i suoi valori, contribuendo a renderlo un individuo complesso e sfaccettato.</li> </ul> <p>In questo modo ho generato <strong>Orlando Marlo, un nobile del regno di Luminaria</strong>. Fervente credente, disprezza la corruzione presente all’interno del regno, in quanto capisce che la religione in cui ha posto la fede è utilizzata dagli altri come leva di potere. Vuole cambiare il sistema dall’interno diventando Ciambellano. E’ un uomo onorevole e molto rispettato, che prova attivamente a costruire un mondo migliore. E’ convinto che chiunque possa essere salvato, ma questa convinzione lo rende paternalista nei confronti di chi non condivide la sua fede.</p> <p>Nelle schede qua sotto è riportato l’intero personaggio.</p> <ul id="orlando-marlo" class="tab" data-tab="c88f7e13-a6e7-4f11-a0b1-ad0860051d48" data-name="orlando-marlo"> <li class="active" id="orlando-marlo-origin-story"> <a href="#">Origin Story </a> </li> <li id="orlando-marlo-values"> <a href="#">Values </a> </li> <li id="orlando-marlo-alignment"> <a href="#">Alignment </a> </li> <li id="orlando-marlo-objectives"> <a href="#">Objectives </a> </li> <li id="orlando-marlo-opinions"> <a href="#">Opinions </a> </li> </ul> <ul class="tab-content" id="c88f7e13-a6e7-4f11-a0b1-ad0860051d48" data-name="orlando-marlo"> <li class="active"> <blockquote> <p>Born into the noble house of Marlo, Orlando grew up witnessing both the splendor and corruption within Luminara’s halls of power. As a child, he experienced a profound spiritual moment when sunlight streamed through the Great Cathedral’s stained glass during his knighting ceremony, filling him with genuine divine purpose. Unlike many nobles who saw faith as a path to power, Orlando’s connection to Solaris became deeply personal and sincere. His father’s deathbed confession about the corruption he witnessed as a Crown Council member strengthened Orlando’s resolve to serve both crown and faith with true righteousness.</p> </blockquote> </li> <li> <blockquote> <ul> <li>Genuine religious devotion and spiritual purity</li> <li>Justice tempered with mercy</li> <li>Protection of the weak, regardless of their social status</li> <li>Honesty and transparency in governance</li> <li>Balance between secular law and divine guidance</li> <li>Personal integrity over political advantage</li> <li>Duty to both crown and faith</li> </ul> </blockquote> </li> <li> <blockquote> <p>Lawful Good</p> </blockquote> </li> <li> <blockquote> <ul> <li>Ascend to the position of Chancellor to influence royal policy</li> <li>Reform the relationship between church and state to serve the people rather than power</li> <li>Create a more equitable system for the marginalized while working within existing structures</li> <li>Prove that true faith and effective governance can coexist without corruption</li> <li>Build bridges between the privileged and the Unblessed through official channels</li> <li>Establish transparency in both Crown Council and Solar Conclave dealings</li> </ul> </blockquote> </li> <li> <blockquote> <h3 id="on-the-order-of-solar-truth">On the Order of Solar Truth</h3> <p>Orlando recognizes the Order’s vital role in preserving sacred knowledge and providing education to the populace - something he deeply values as a foundation of a stable society. He admires their dedication to maintaining the ancient texts and their role in bringing literacy to even remote villages. However, he is troubled by how the Order increasingly manipulates religious doctrine for political gain. Their practice of withholding certain teachings from the “Unblessed” particularly disturbs him, as he believes Solaris’s light should shine on all equally. While he would never speak openly against them, he silently disapproves of how they’ve transformed from spiritual guides into power brokers, using their control over education to shape political narratives. He sees their current path as a corruption of their original sacred mission, though he struggles with this critique, knowing the importance of maintaining religious authority in Luminara’s governance.</p> <h3 id="on-sols-universal-message">On Sol’s Universal Message</h3> <p>Orlando finds profound inspiration in Sol’s sermon of the Shared Dawn, where the prophet stood on Mount Luminous and proclaimed that just as the morning sun touches both palace and hovel alike, divine grace knows no walls or borders. This message resonates deeply with Orlando, who often meditates on how Sol compared human-made hierarchies to clouds that foolishly believe they can block the sun’s light permanently. The knight particularly cherishes the prophet’s warning that “those who claim to own the light cast the darkest shadows,” seeing it as a perfect metaphor for the current corruption in Luminara’s institutions. When alone in the castle gardens at sunrise, Orlando often recreates Sol’s famous gesture of open arms toward the sun, reflecting on how this simple yet powerful truth about universal divine love has been twisted by those who claim to be its guardians. The contradiction between Sol’s original teachings and the current practice of marking some as “Unblessed” represents, to Orlando, the greatest betrayal of their prophet’s vision.</p> </blockquote> </li> </ul> <h2 id="modellazione">Modellazione</h2> <p>Tutto quello che è stato discusso fino ad adesso riguarda il tipo di dati e informazioni che il designer del personaggio deve fornire.</p> <p>Una volta raccolte queste informazioni, può iniziare l’allenamento dell’LLM. La prima cosa da fare è la creazione di un dataset di conversazioni del personaggio. Idealmente anche queste dovrebbero essere fornite dal designer.</p> <h3 id="generazione-delle-domande">Generazione delle domande</h3> <p>Per generare delle domande, ho deciso di utilizzare Gemma2 2B, un modello di Google di dimensioni abbastanza ridotte con capacità dignitose. Non ho scelto un modello più grande poiché dalle prove sembrava funzionare bene.</p> <p>Per qualsiasi tipo di generazione, bisogna creare un prompt per Gemma, ovvero delle istruzioni che il modello deve seguire. Un prompt include:</p> <ul> <li><strong>Persona</strong>: chi è il modello e che personalità deve avere.</li> <li><strong>Istruzioni</strong>: cosa deve fare il modello.</li> <li><strong>Esempi</strong>: opzionali, sono degli esempi da mostrare al modello per condizionare meglio la generazione.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GenerativePrompt</span><span class="p">:</span>
    <span class="n">persona</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">instruction</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
</code></pre></div></div> <p>Per generare le domande, ho adottato un approccio che introduce variabilità attraverso personaggi casuali. Ho creato una decina di personaggi con descrizioni abbastanza scarne (ad esempio <strong>“Elena Solwind, royal historian, scholarly and reserved”</strong>).</p> <p>Per aumentare ulteriormente la variabilità nelle interazioni, ho aggiunto:</p> <ul> <li>Un’emozione casuale assegnata al personaggio per ogni conversazione</li> <li>Un contesto situazionale che include: <ul> <li>Il luogo dell’incontro</li> <li>Il motivo dell’interazione</li> <li>Il livello di intimità tra i personaggi</li> </ul> </li> </ul> <p>Nelle sezione <strong>instruction</strong> ho inserito:</p> <ul> <li>La backstory completa di Orlando Marlo</li> <li>Opzionalmente, una delle sue opinioni</li> <li>I dettagli contestuali generati</li> </ul> <p>Questo approccio permette di ottenere domande che variano naturalmente in base al background del personaggio (uno storico porrà domande diverse da un capo gilda) e al rapporto che il personaggio ha con Orlando.</p> <p>Ecco un esempio di prompt:</p> <blockquote> <p>You are <strong>Elena Solwind, royal historian, scholarly and reserved</strong>. You are feeling <strong>calm</strong>.</p> <p>You will be given some information about me. I’m a character from a fantasy world.</p> <p><em>…</em></p> <p><em>… backstory di Orlando Marlo …</em></p> <p><em>…</em></p> <p><strong>Meeting location</strong>: The Great Cathedral of Solaris</p> <p><strong>Meeting reason</strong>: delivering urgent message</p> <p><strong>Knowledge of my background</strong>: high</p> <p>We are going to have a conversation. You will ask me a question.We are having a direct conversation. Do not use indirect speech.</p> <p>… <em>examples of questions</em> …</p> </blockquote> <p>Ora che ho completato questo progetto, posso dire che non sono particolarmente soddisfatto delle domande generate. Il modello Gemma2 2B ha mostrato una tendenza a produrre output verbosi e poco naturali, spesso premettendo un riassunto completo della backstory di Orlando prima di formulare la domanda vera e propria. Questa performance sub-ottimale potrebbe essere attribuita a diverse cause:</p> <ul> <li><strong>Un sovraccarico informativo nel prompt</strong>, con la backstory completa che potrebbe aver “distratto” il modello dal suo obiettivo principale</li> <li><strong>Le limitazioni strutturali di Gemma2B</strong>, che non supporta un prompt di sistema separato, costringendo a includere tutte le istruzioni nel messaggio dell’utente</li> <li><strong>Possibili inefficienze nella struttura del prompt stesso</strong> che non ho avuto modo di perfezionare</li> </ul> <ul id="group-name" class="tab" data-tab="7822383d-cca4-4805-a832-40110085fd29" data-name="group-name"> <li class="active" id="group-name-elena-solwind"> <a href="#">Elena Solwind </a> </li> <li id="group-name-isabella-stormwind"> <a href="#">Isabella Stormwind </a> </li> </ul> <ul class="tab-content" id="7822383d-cca4-4805-a832-40110085fd29" data-name="group-name"> <li class="active"> <blockquote> <p><strong>Elena Solwind, royal historian, scholarly and reserved</strong>, meeting in <strong>Healing Sanctuaries</strong>:</p> <p>Orlando Marlo… a name well-known even amongst those who reside outside the realm of Luminara’s grandeur. Tell me, what is your assessment of Seraphina’s vision regarding the kingdom’s future? Do you believe her words carry an inherent warning or merely political commentary?</p> </blockquote> </li> <li> <blockquote> <p><strong>Commander Isabella Stormwind, border patrol leader, pragmatic and direct</strong>, meeting in <strong>Marlo Family Estate</strong>:</p> <p>Orlando Marlo. I hear whispers of your family, their long history intertwined with this land. But what drove them - specifically <em>you</em> - towards a life dedicated to upholding both Solaris and the crown?</p> </blockquote> </li> </ul> <h3 id="generazione-delle-risposte">Generazione delle risposte</h3> <p>Il processo di generazione delle risposte segue un approccio simile alla generazione delle domande. In questo caso ho usato Llama3.1 8B (quantizzato a 4 bit).</p> <p>Per la <strong>persona</strong>, ho utilizzato Orlando Marlo includendo:</p> <ul> <li>La sua backstory completa</li> <li>Una serie di motti e frasi ricorrenti tipiche del personaggio (non tanto per caratterizzarlo, quanto per verificare se il modello riesce a imparare questi pattern dopo l’allenamento)</li> </ul> <p>Il processo di generazione è semplice:</p> <ul> <li>Il messaggio dell’utente è la domanda generata nello step precedente</li> <li>Il modello risponde nei panni di Orlando</li> </ul> <p>Gli esempi di risposte in questa fase sono stati cruciali per catturare la personalità di Orlando. Sono stati selezionati con un processo iterativo:</p> <ol> <li>Selezione di una domanda casuale dal dataset.</li> <li>Valutazione della risposta generata da Orlando.</li> <li>Modifica della risposta per ridurre elementi generici e enfatizzare lo stile caratteristico del personaggio</li> <li>Aggiunta della risposta modificata al set di esempi per migliorare le generazioni successive</li> </ol> <p>Ecco un esempio del prompt, prima di essere riempito con le variabili importanti.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
{% persona %}

{% instruction %}

You are talking with: {% character %}

Meeting location: {% location %}

{% if examples %}
Here</span><span class="sh">'</span><span class="s">s some examples of past conversations between you and other characters.
Use this examples as a style guide.
{% for example in examples %}
## Example {loop.index}
{example}
{% endfor %}
{% endif %}

Use only direct speech. No description of tone.
Answer the question referencing the information about you that you know.
Don</span><span class="sh">'</span><span class="s">t introduce yourself.
Don</span><span class="sh">'</span><span class="s">t be cringe, you are having a conversation with a real person.
</span><span class="sh">"""</span>
</code></pre></div></div> <h2 id="allenamento">Allenamento</h2> <p>Ho scelto di utilizzare <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct">SmolLM2</a>, uno dei modelli linguistici più compatti disponibili, per questo esperimento. Esiste in tre varianti (135M, 360M e 1.7B parametri) e ho optato per la versione da 135M. Il modello ha un contesto di 2048 token, una limitazione accettabile per questo test, ma che potrebbe diventare problematica per future conversazioni multi-turno.</p> <p>L’obiettivo era finetunare il modello sul dataset di circa mille esempi generati precedentemente. Durante i primi tentativi ho scoperto un problema interessante specifico di questi modelli: il token di PAD (usato per uniformare la lunghezza delle sequenze) coincide con il token EOS (fine sequenza). Questo causa un comportamento indesiderato, in cui il modello “dimentica” come terminare le risposte e non smette mai di parlare.</p> <p>Per l’allenamento ho utilizzato <a href="https://github.com/huggingface/trl">TRL</a> con i seguenti parametri:</p> <ul> <li><strong>learning rate</strong>: 5e-5 (volutamente conservativo)</li> <li><strong>epoche</strong>: 5 (il modello migliore è stato quello della quarta epoca)</li> </ul> <p>Un’ottimizzazione importante è stata l’utilizzo dei <a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a>, che mi hanno permesso di raddoppiare la batch size da 4 a 8, riducendo significativamente i tempi di training grazie a una gestione più efficiente della memoria. Ho sperimentato con due approcci diversi:</p> <ol> <li><strong>Includendo la backstory di Orlando nel contesto di training</strong>: l’allenamento è molto più rapido in quanto tutte le informazioni sono già presenti nel prompt</li> <li><strong>Senza includere informazioni specifiche nel contesto</strong>: la loss del modello è più alta, in quanto il modello ha dovuto imparare implicitamente le informazioni su Orlando. Il vantaggio di questo approccio è che il contesto è molto più compatto e può quindi essere usato per codificare altre conversazioni.</li> </ol> <p>Le prove sul modello allenato hanno mostrato risultati interessanti: riesce a utilizzare correttamente le frasi caratteristiche di Orlando e mantiene una buona coerenza quando risponde a domande simili a quelle del training. Tuttavia, emerge un limite significativo quando il modello si trova di fronte a domande che si discostano troppo dal dataset di allenamento - in questi casi tende a ignorare la domanda e a ripetere informazioni su se stesso in modo poco naturale.</p> <p>Queste osservazioni suggeriscono due direzioni principali per migliorare il sistema: da un lato, sarà fondamentale espandere la varietà del dataset di training per coprire un range più ampio di possibili interazioni. Dall’altro, sarà cruciale ridurre la dipendenza da conversazioni generate automaticamente e incorporare invece più esempi di dialoghi creati da esseri umani, che tipicamente presentano sfumature e complessità difficili da replicare attraverso la generazione automatica.</p> <h2 id="conclusioni-e-sviluppi-futuri">Conclusioni e Sviluppi Futuri</h2> <p>Questo primo prototipo di LeMoNPC è stata per me più una scusa per sperimentare con la generazione sintetica di dati, modellare un LLM per fare roleplay e infine per mettere le mani in pasta facendo un finetuning.</p> <p>La generazione automatica delle domande ha mostrato diverse limitazioni, probabilmente dovute ad un cattivo prompt engineering. In futuro dovrò trovare un modo migliore per modellare questo aspetto. Sono invece rimasto molto stupito dalla capacità di Llama3.1 di fare roleplay e generare delle risposte abbastanza coerenti. Di sicuro la vera qualità verrà da dati creati e curati da esseri umani.</p> <p>Per quel che riguarda il finetuning, lo considero un successo. È vero che fuori distribuzione il modello risponde in maniera strana, ma questo è più un problema sui dati che sul modello. In futuro mi piacerebbe applicare Direct Preference Optimization (DPO) per rafforzare il training. Ovviamente questo complicherà in maniera aggiuntiva la raccolta di un dataset, ma di sicuro sarà divertente.</p>]]></content><author><name></name></author><category term="llm"/><category term="roleplay"/><category term="llm"/><category term="ai"/><category term="roleplay"/><summary type="html"><![CDATA[Un framework per convertire personaggi in Language Models]]></summary></entry><entry><title type="html">Imparando Manim</title><link href="https://oscarpindaro.github.io/blog/2024/esperimenti-manim/" rel="alternate" type="text/html" title="Imparando Manim"/><published>2024-11-25T18:00:00+00:00</published><updated>2024-11-25T18:00:00+00:00</updated><id>https://oscarpindaro.github.io/blog/2024/esperimenti-manim</id><content type="html" xml:base="https://oscarpindaro.github.io/blog/2024/esperimenti-manim/"><![CDATA[<p>Come si può intuire dal titolo, questo post non ha un vero proprio tema, ma è più una raccolta di animazioni che ho fatto per diventare un po’ più disinvolto con <a href="https://www.manim.community/">Manim</a>, la libreria python che utilizzo per creare le gif del blog.</p> <h2 id="cosè-manim">Cos’è Manim</h2> <p>Manim (Mathematical Animation Engin) è una libreria python progettata per creare animazioni matematiche in maniera programmatica.</p> <p>La trovo molto comoda perchè sono un pessimo artista ma mi trovo molto a mio agio a scrivere codice.</p> <p>L’autore di questa libreria è Grant Sanderson, uno dei più famosi divulgatori di matematica. La libreria è nata inizialmente da un suo progetto personale, in cui pubblica video divulgativi suo canale YouTube <a href="https://www.youtube.com/c/3blue1brown">3Blue1Brown</a>.</p> <p>Le animazioni che vedrete sono abbastanza semplici. L’obiettivo è diventare abbastanza disinvolto nell’utilizzo della libreria, e allo stesso tempo trovare una palette che mi permetta di distanziarmi dallo stile di <em>3Blue1Brown</em>.</p> <p>Per adesso ci sono pochi esempi, ma spero di aggiungerne altri in futuro.</p> <h2 id="stella-di-linee">Stella di linee</h2> <p>Questa è una sorta di illusione ottica. Anche se per disegnare questa stella si usano solo linee rette, il risultato sembra una stella con i lati curvi, che ricordano rami di iperboli. Per costruire questa stella bisogna:</p> <ol> <li>Scegliere due assi adiacenti (ad esempio l’asse x positivo e y positivo).</li> <li>Dividere entrambi gli assi in N parti.</li> <li>Partire dal punto più lontano dall’origine su un asse e collegarlo al punto più vicino sull’altro asse.</li> <li>Disegnare la retta successiva avvicinandosi sull’asse su cui si era più lontani e allontanandosi su quello su cui si era più vicini.</li> </ol> <h3 id="palette-chiare">Palette chiare</h3> <p>Le palette chiare sono state quelle più difficili da creare. L’idea originale era quella di avere tante stelle con i colori dell’arcobaleno: tuttavia non tutte si mischiano bene con uno sfondo chiaro.</p> <p>In particolare, l’arcobaleno con i colori pastello è in assoluto il peggiore. Riflettendoci meglio il motivo è abbastanza chiaro: essendo i colori pastellati molto vicini al bianco, tenderanno a mischiarsi meglio con uno sfondo scuro.</p> <p>I miei preferiti sono le animazioni con le palette calde, che ricordano il tramonto.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_golden_sunset_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_golden_sunset_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_golden_sunset_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_golden_sunset_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_onedark_classic_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_onedark_classic_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_onedark_classic_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_onedark_classic_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_pastel_rainbow_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_pastel_rainbow_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_pastel_rainbow_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_pastel_rainbow_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_high_saturation_rainbow_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_high_saturation_rainbow_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_high_saturation_rainbow_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_high_saturation_rainbow_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_onedark_class_rainbow_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_onedark_class_rainbow_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_onedark_class_rainbow_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_onedark_class_rainbow_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_sunset_skyline_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_sunset_skyline_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_sunset_skyline_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_sunset_skyline_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_muted_rainbow_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_muted_rainbow_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_muted_rainbow_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_muted_rainbow_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_neon_rainbow_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_neon_rainbow_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_neon_rainbow_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_neon_rainbow_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/light/LinestarScene_onedark_f0ead6-480.webp 480w,/assets/gif/linestar/light/LinestarScene_onedark_f0ead6-800.webp 800w,/assets/gif/linestar/light/LinestarScene_onedark_f0ead6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/light/LinestarScene_onedark_f0ead6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h3 id="palette-scure">Palette scure</h3> <p>Queste sono le palette che mi hanno dato in assoluto più soddisfazioni, quindi godetevele.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_golden_sunset_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_golden_sunset_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_golden_sunset_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_golden_sunset_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_onedark_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_onedark_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_onedark_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_onedark_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_pastel_rainbow_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_pastel_rainbow_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_pastel_rainbow_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_pastel_rainbow_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_high_saturation_rainbow_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_high_saturation_rainbow_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_high_saturation_rainbow_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_high_saturation_rainbow_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_onedark_classic_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_onedark_classic_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_onedark_classic_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_onedark_classic_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_sunset_skyline_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_sunset_skyline_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_sunset_skyline_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_sunset_skyline_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_muted_rainbow_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_muted_rainbow_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_muted_rainbow_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_muted_rainbow_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_neon_rainbow_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_neon_rainbow_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_neon_rainbow_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_neon_rainbow_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_onedark_class_rainbow_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_onedark_class_rainbow_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_onedark_class_rainbow_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_onedark_class_rainbow_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/linestar/dark/LinestarScene_onedark_vivid_rainbow_282c33-480.webp 480w,/assets/gif/linestar/dark/LinestarScene_onedark_vivid_rainbow_282c33-800.webp 800w,/assets/gif/linestar/dark/LinestarScene_onedark_vivid_rainbow_282c33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/linestar/dark/LinestarScene_onedark_vivid_rainbow_282c33.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="polinomi">Polinomi</h2> <p>Queste animazioni erano per capire come utilizzare il <code class="language-plaintext highlighter-rouge">NumberPlane</code> (Piano Cartesiano) di manim. Purtroppo ci sono ancora alcune sbavature (le funzioni vengono plottate anche fuori dai limiti degli assi). Devo ancora capire se c’è qualche parametro da settare oppure bisogna solo essere un po’ smart quando si disegnano le funzioni.</p> <p>Durante lo sviluppo tendo a fare animazioni molto rapide, ma ora che le riguardo mi rendo conto che bisogna lasciar tempo all’osservatore di capire cosa sta succedendo sullo schermo.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/manim-experiments/StraightLinePoly_ManimCE_v0.18.1-480.webp 480w,/assets/gif/manim-experiments/StraightLinePoly_ManimCE_v0.18.1-800.webp 800w,/assets/gif/manim-experiments/StraightLinePoly_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/manim-experiments/StraightLinePoly_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Plot di due rette. </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/manim-experiments/ParabolePoly_ManimCE_v0.18.1-480.webp 480w,/assets/gif/manim-experiments/ParabolePoly_ManimCE_v0.18.1-800.webp 800w,/assets/gif/manim-experiments/ParabolePoly_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/manim-experiments/ParabolePoly_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Plot di due parabole, una convessa e una concava. I colori di questa palette mi piacciono molto. </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/manim-experiments/PolynomialTransformation_ManimCE_v0.18.1-480.webp 480w,/assets/gif/manim-experiments/PolynomialTransformation_ManimCE_v0.18.1-800.webp 800w,/assets/gif/manim-experiments/PolynomialTransformation_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/manim-experiments/PolynomialTransformation_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Trasformazione da un polinomio di quarto grado a uno di terzo. La palette originale di 3Blue1Brown mantiene sempre il suo fascino. </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/manim-experiments/RandomCubics_ManimCE_v0.18.1-480.webp 480w,/assets/gif/manim-experiments/RandomCubics_ManimCE_v0.18.1-800.webp 800w,/assets/gif/manim-experiments/RandomCubics_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/manim-experiments/RandomCubics_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Una serie di cubiche casuali. Ho usato una palette dai colori caldi del tramonto. Provo sensazioni miste, probabilmente il nero dovrebbe essere meno saturo. Inoltre i rossi usati per le funzioni sono troppo vicini nello spettro. </div> <h2 id="regressione-lineare">Regressione Lineare</h2> <p>Questo è un esempio di regressione lineare, utilizzando però solo feature polinomiali. I dati sono campionati dalla distribuzione rossa, un polinomio di quarto grado. Successivamente, un polinomio di terzo grado è fittato usando Stochastic Gradient Descent.</p> <p>La regressione lineare è un <em>problema a forma chiusa</em>, ovvero esiste una soluzione che può essere calcolata analiticamente senza alcuna approssimazione dell’errore. Tuttavia, la Discesa del Gradiente ha un nome molto più divertente ed è anche più bello da animare.</p> <p>I colori lasciano un po’ a desiderare, ma avevo già perso troppo tempo a giocherellare con i parametri della regressione.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/manim-experiments/PolynomialFitting_ManimCE_v0.18.1-480.webp 480w,/assets/gif/manim-experiments/PolynomialFitting_ManimCE_v0.18.1-800.webp 800w,/assets/gif/manim-experiments/PolynomialFitting_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/manim-experiments/PolynomialFitting_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Dei punti vengono campionati dalla distribuzione rossa. In bianco, i diversi polinomi di terzo grado che vengono creati durante la Discesa del gradiente. Più passa il tempo, meglio approssimano la funzione originale </div> <ul id="gradient-descent" class="tab" data-tab="25f5b094-69ae-4b4b-bfa5-f8945a51ca40" data-name="gradient-descent"> <li class="active" id="gradient-descent-latex"> <a href="#">latex </a> </li> <li id="gradient-descent-code"> <a href="#">code </a> </li> </ul> <ul class="tab-content" id="25f5b094-69ae-4b4b-bfa5-f8945a51ca40" data-name="gradient-descent"> <li class="active"> \[\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} f(\theta_t)\] <p>Dove:</p> <ul> <li>\(\theta\) rappresenta i parametri della regressione (ovvero i coefficienti del polinomio),</li> <li>\(\alpha\) è il tasso di apprendimento (cioè quanto del gradiente considerare durante l’aggiornamento dei parametri),</li> <li>\(\nabla_{\theta}f(\theta_t)\) è il gradiente dell’errore rispetto ai parametri della regressione. Se il regressore è un polinomio di grado 3, allora il gradiente sarà un vettore di 4 elementi, uno per ciascun coefficiente del polinomio. Il valore del gradiente per un parametro corrisponde al valore della potenza associata a quel parametro.</li> </ul> </li> <li> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">gradient_ascent_fit</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
      <span class="kn">import</span> <span class="n">math</span>

      <span class="n">errors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">fitted_polys</span> <span class="o">=</span> <span class="p">[]</span>

      <span class="c1"># if the gradients become too big, the loop may become numerically unstable
</span>      <span class="n">clip_val</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">clip_val0</span>

      <span class="c1"># a preliminary solution can be a random polynomial
</span>      <span class="n">estimator</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">.</span><span class="nf">random</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_degree</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
      <span class="c1"># estimator = Polynomial((1,)*5)
</span>
      <span class="c1"># first iteration
</span>      <span class="n">fitted_polys</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>
      <span class="n">errors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">.</span><span class="nf">compute_error</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">y_true</span><span class="p">))</span>

      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_steps</span><span class="p">):</span>
          <span class="c1"># the learning weight gets smaller the more time passes. It helps convergence
</span>          <span class="n">lr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">lr0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
          <span class="c1"># lr = self.lr0
</span>          <span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">gradient_ascent_step</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">y_true</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">clip_val</span><span class="p">)</span>
          <span class="n">fitted_polys</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>
          <span class="n">errors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">.</span><span class="nf">compute_error</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">y_true</span><span class="p">))</span>

      <span class="k">return</span> <span class="n">fitted_polys</span><span class="p">[::</span><span class="n">self</span><span class="p">.</span><span class="n">save_every</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">fitted_polys</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">errors</span><span class="p">[::</span><span class="n">self</span><span class="p">.</span><span class="n">save_every</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">errors</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
</code></pre></div></div> <ul> <li>Inizializzazione: Si crea un polinomio casuale e si calcola il suo errore iniziale.</li> <li>Iterazioni: Per un certo numero di passi, il polinomio viene aggiornato usando gradient ascent (spostandosi nella direzione del gradiente per ridurre l’errore). Il learning rate diminuisce nel tempo per favorire la convergenza.</li> <li>Output: Restituisce i polinomi e gli errori ad intervalli regolari e alla fine.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="animations"/><category term="code"/><summary type="html"><![CDATA[Una raccolta di animazioni per imparare ad usare meglio manim]]></summary></entry><entry><title type="html">Algoritmi Genetici e le 8 regine</title><link href="https://oscarpindaro.github.io/blog/2024/algoritmi-genetici/" rel="alternate" type="text/html" title="Algoritmi Genetici e le 8 regine"/><published>2024-09-30T18:00:00+00:00</published><updated>2024-09-30T18:00:00+00:00</updated><id>https://oscarpindaro.github.io/blog/2024/algoritmi-genetici</id><content type="html" xml:base="https://oscarpindaro.github.io/blog/2024/algoritmi-genetici/"><![CDATA[<h2 id="introduzione">Introduzione</h2> <p>Gli <em>algoritmi genetici</em> sono una classe di algoritmi utilizzati per prototipare rapidamente soluzioni a problemi complessi. Il loro funzionamento si ispira al processo di <em>selezione naturale</em>, in cui gli individui di una popolazione continuano a cambiare e a riprodursi. Solo gli individui più adatti riescono a sopravvivere.</p> <p>Godono di grande popolarità, in quanto il loro funzionamento è molto intuitivo e sono molto semplici da progettare e implementare. Infatti, permettono di prototipare rapidamente soluzioni di qualità discreta a problemi più o meno difficili.</p> <p>Per spiegare il loro funzionamento, userò come esempio il problema delle 8 regine.</p> <h2 id="problema-delle-8-regine">Problema delle 8 regine</h2> <p><a href="https://it.wikipedia.org/wiki/Rompicapo_delle_otto_regine#:~:text=Il%20rompicapo%20(o%20problema)%20delle,i%20movimenti%20standard%20della%20regina.">Il problema delle 8 regine</a> è un puzzle che consiste nel posizionare 8 regine su una scacchiera 8x8 senza che nessuna di queste minacci le altre.<br/> La “difficoltà” del problema consiste nel fatto che le regine minacciano gli altri pezzi che si trovano sulla stessa riga, colonna o diagonali. Tradizionalmente il problema viene risolto utilizzando un algoritmo <em>Depth-First Search (DFS)</em> di backtracking.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/gen_alg_eight_queens/QueenAttacking_ManimCE_v0.18.1-480.webp 480w,/assets/gif/gen_alg_eight_queens/QueenAttacking_ManimCE_v0.18.1-800.webp 800w,/assets/gif/gen_alg_eight_queens/QueenAttacking_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/gen_alg_eight_queens/QueenAttacking_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> La regina è un pezzo molto aggressivo. Può attaccare in orizzontale, verticale e diagonale e muoversi di quanti passi vuole. </div> <p>In DFS, le regine vengono posizionate una alla volta, in maniera tale che l’ultima regina non minacci nessuna delle precedenti, fino a quando non si trova una soluzione o non è possibile posizionare una regina. In questo ultimo caso, l’ultima mossa viene annullata, e si prova in maniera ricorsiva a cambiare le posizioni delle precedenti regine fino alla risoluzione. Questo algoritmo è basato su <em>state space search (ricerca nello spazio di stato)</em>, in cui vengono provate tutte le possibili configurazioni.</p> <p>Il problema delle 8 regine può essere generalizzato a N regine su una scacchiera NxN. Tuttavia, la complessità cresce esponenzialmente, e calcolare una soluzione con DFS diventa computazionalmente intrattabile.</p> <h2 id="componenti-di-un-algoritmo-genetico">Componenti di un algoritmo genetico</h2> <p>Gli algoritmi genetici considerano possibili <em>soluzioni</em> ad un problema come individui in una popolazione. In questo caso, la parola <em>soluzione</em> non assume il significato classico di <em>risoluzione</em>, ovvero quella soluzione che risolve il problema. Ha invece una accezione molto più ampia: è infatti un qualsiasi assegnamento alle variabili del problema. Esistono quindi soluzioni di qualità più o meno alta, ammissibili e inammissibili, e l’obiettivo dell’algoritmo genetico è quello di trovare una soluzione di qualità abbastanza alta (possibilmente la migliore).</p> <p>In un algoritmo genetico, gli individui possono mutare spontaneamente o riprodursi l’un l’altro. Tuttavia, solo gli individui migliori riescono a sopravvivere e a generare “soluzioni” figlie di qualità sempre migliori.</p> <p>I componenti di un algoritmo genetico sono:</p> <ul> <li>La funziona di fitness</li> <li>La rappresentazione delle soluzione</li> <li>La mutazione</li> <li>Il crossover</li> <li>La popolazione</li> </ul> <h3 id="funzione-di-fitness">Funzione di Fitness</h3> <p>La funziona di Fitness è una funzione che assegna un punteggio ad una particolare soluzione. Questa funzione può essere complicata a piacere, e può contenere penalizzazioni per alcuni casi patologici o forti premi per caratteristiche desiderabili.</p> <p>La funzione di fitness condiziona quanto una soluzione riesce a sopravvivere tra una generazione e l’altra. Infatti, gli individui con fitness più alta sono quelli che avranno più probabilità di riprodursi e passare i propri geni positivi alle prossime generazione. In questa maniera soluzioni con fitness bassa hanno una bassa probabilità di generare figli, mentre le migliori soluzioni riescono a generare anche multipli figli con soluzioni altrettanto buone.</p> <h3 id="rappresentazione-della-soluzione">Rappresentazione della soluzione</h3> <p>La <em>rappresentazione</em> è l’astrazione che sintetizza i dettagli essenziali del problema, ed è realizzata attraverso una specifica <em>struttura dati</em>. Questa scelta è cruciale perché influisce direttamente sulle operazioni che possono essere eseguite durante la mutazione e il crossover, insieme all’efficienza con cui tali operazioni vengono eseguite.</p> <p>Ad esempio, nel caso del problema delle 8 regine, la scacchiera potrebbe essere rappresentata come una lista di quadrati, una lista di righe o persino come un grafo di quadrati collegati. Ogni struttura dati offre vantaggi e svantaggi a seconda delle operazioni richieste: alcune possono facilitare certi tipi di mutazioni o controlli di validità, mentre altre potrebbero renderli più complessi o inefficienti.</p> <p>Scegliere la giusta rappresentazione facilita la ricerca di soluzioni di qualità e può accelerare notevolmente il processo evolutivo all’interno dell’algoritmo genetico. Al contrario, una rappresentazione inadeguata può portare alla generazione di soluzioni inammissibili, rallentando l’intero processo evolutivo e facendo perdere individui potenzialmente validi.</p> <h3 id="mutazione">Mutazione</h3> <p>La <em>mutazione</em> è una perturbazione casuale del genoma dell’individuo. Questo è un processo che serve a creare nuove soluzioni da una soluzione pre-esistente, senza cambiarla troppo. L’idea di fondo è che se l’individuo originale ha già una buona fitness,anche individui “adiacenti” avranno una fitness simile. In questa maniera è molto semplice trovare la soluzione ottimale nel caso si abbia già un individuo molto promettente.</p> <p>Nel caso il genoma sia composto da numeri, la soluzione può essere una piccola perturbazione. Nel caso invece il genoma contenga valori categorici (come ad esempio “rosso”, “giallo”, “blu”), basta scegliere casualmente uno degli altri valori.</p> <p>La mutazione è un processo stocastico. Il designer dell’algoritmo genetico deve decidere quanto alta deve essere la probabilità di mutazione. Se troppo bassa, nella popolazione saranno presenti sempre gli stessi individui. Se troppo alta invece la popolazione cambierà troppo frequentemente e le soluzioni ottimali verranno diluite.</p> <h3 id="crossover">Crossover</h3> <p>In biologia il <em>crossover</em> è il processo per cui durante la produzione dei gameti, i cromosomi materni e paterni si intersecano e mischiano. Questo permette ad un individuo di creare gameti molto più diversi da sé stesso, aumentando la variabilità genetica e la probabilità di creare della prole migliore.</p> <p>Negli algoritmi genetici il crossover è molto simile. Infatti le soluzioni di due diversi individui vengono mischiati, e due nuovi individui con caratteristiche di entrambi i genitori vengono creati. Mentre la mutazione causa delle piccole perturbazioni locali attorno ad una soluzione, il crossover permette di generare individui molto diversi da quelli originali e superare quindi minimi locali per trovare soluzioni ottime globalmente. Il crossover è così efficace perché permette a individui molto promettenti di mischiarsi e generare con alta probabilità una soluzione molto migliore, ma permette anche a individui con poca fitness ma con alcune caratteristiche vincenti di unirsi a soluzioni di buona qualità.</p> <h2 id="applicazione-al-problema-delle-8-regine">Applicazione al problema delle 8 Regine</h2> <p>Per il problema delle 8 regine, ho deciso di usare come <em>funzione di fitness</em> il numero di regine minacciate da altre regine. L’obiettivo è quindi quello di minimizzare la fitness. Infatti, l’individuo ottimale avrà fitness pari a 0, in quanto nessuna delle sue regine minaccia le altre.</p> <p>Per quel che riguarda <em>la rappresentazione della soluzione</em>, una scacchiera è rappresentata come una matrice di dimensione NxN i cui elementi possono essere 0 o 1. Se una cella ha valore 0, allora è vuota, altrimenti contiene una regina. Per generare una scacchiera casuale, si potrebbero pescare a caso 8 celle e posizionare le regine. Questo è un esempio di <em>cattiva rappresentazione</em>, in quanto può selezionare con alta frequenza regine sulla stessa riga, colonna o diagonale. Per ovviare a questo problema, si può posizionare una regina su ciascuna riga. In questa maniera si è sicuri che le regine possono essere attaccate solo verticalmente o diagonalmente, riducendo drasticamente il numero di cattive soluzioni.</p> <p>Ora che è stato definita la rappresentazione, è abbastanza ovvio come scegliere mutazione e crossover. Il processo di <em>mutazione</em> seleziona una riga a caso e cambia la posizione della regina, sperando che lo spostamento orizzontale sia abbastanza per migliorare la fitness. Questo mutazione può essere considerato un <em>bit-flip</em>, in quanto la scacchiera rappresenta le posizioni in bit e lo spostamento della regina consiste nel cambiamento di due celle da 0 a 1 e viceversa.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/gen_alg_eight_queens/QueenMutating_ManimCE_v0.18.1-480.webp 480w,/assets/gif/gen_alg_eight_queens/QueenMutating_ManimCE_v0.18.1-800.webp 800w,/assets/gif/gen_alg_eight_queens/QueenMutating_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/gen_alg_eight_queens/QueenMutating_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Una sequenza di mutazioni. La fitness può migliorare, peggiorare o rimanere invariata. Essendo una piccola perturbazione, la fitness non cambia drasticamente, ma rimane nell'intorno del valore originale. </div> <p>Il <em>crossover</em> invece consiste nel scegliere uno a più righe a caso e scambiarle tra due diverse scacchiere. Esistono altri tipi di crossover per matrici in cui, ad esempio, si considerano sub-matrici quadrate o rettangolari. Tuttavia la rappresentazione che è stata scelta permette di utilizzare la prima versione, più semplice, che inoltre continua a garantire la presenza di una sola regina per riga.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/gif/gen_alg_eight_queens/QueenCrossover_ManimCE_v0.18.1-480.webp 480w,/assets/gif/gen_alg_eight_queens/QueenCrossover_ManimCE_v0.18.1-800.webp 800w,/assets/gif/gen_alg_eight_queens/QueenCrossover_ManimCE_v0.18.1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/gif/gen_alg_eight_queens/QueenCrossover_ManimCE_v0.18.1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption" style="font-size: 18px; font-style: italic;"> Un crossover tra due scacchiere. Il numero di righe selezionate è arbitrario, e può essere fino a N-1 (N in questo caso è 8). </div> <h2 id="pro-e-contro-degli-algoritmi-genetici">Pro e Contro degli Algoritmi Genetici</h2> <p>Come tutte le tecniche, gli Algoritmi Genetici non sono il Sacro Gral, e bisogna valutare di volta in volta se sono o meno lo strumento corretto.</p> <h3 id="pro">Pro</h3> <p>Il grande vantaggio degli algoritmi genetici è la loro flessibilità. Il design della soluzione si affida molto all’intuito del progettista, che potrebbe già avere un’idea di quale forma debba avere una buona soluzione. Questo rende anche la fase di design molto leggera, poiché il codice da scrivere non è particolarmente complicato e non richiede l’uso di framework o librerie complesse. È senza dubbio un ottimo modo per incorporare all’interno di un modello la propria intuizione, ed è il motivo per cui è così accessibile.</p> <p>Un altro grande vantaggio è l’elevata velocità di prototipizzazione. In caso di scadenze molto strette, rappresentano un buon modo per produrre una soluzione decente e funzionante, rimandando l’adozione di tecniche più sofisticate ad un momento successivo.</p> <p>Infine, le regole di mutazione e crossover possono essere definite per qualsiasi tipo di dato, senza particolari vincoli. Questo consente di descrivere soluzioni con strutture esotiche, difficilmente o per nulla riproducibili con altri tipi di algoritmi. Offre anche grande flessibilità nella definizione di vincoli e soluzioni non ammissibili, che possono essere penalizzate nella funzione di fitness o scartate a priori quando vengono generate.</p> <h3 id="contro">Contro</h3> <p>Il processo di ottimizzazione negli Algoritmi Genetici dipende fortemente dalla robustezza e dalla qualità della <em>rappresentazione</em> scelta. La <em>funzione di fitness</em> è cruciale, e spesso in un progetto è necessario iterare diverse versioni della funzione prima di trovarne una soddisfacente. Inoltre, non sempre le <em>mutazioni e i crossover</em> selezionati sono efficaci nel superare minimi locali poco soddisfacenti.</p> <h3 id="possibili-alternative">Possibili Alternative</h3> <p>I contro derivano dal fatto che gli Algoritmi Genetici non imparano esplicitamente dagli errori che commettono, ma necessitano di molti tentativi per esplorare efficacemente lo spazio di stato.</p> <p>Esistono altri algoritmi in grado di sfruttare una caratteristica molto utile di alcune funzione di fitness chiamata <em>differenziabilità</em>. Una funzione di fitness differenziabile infatti non solo comunica la qualità di una soluzione, ma è anche in grado di descrivere come la soluzione deve cambiare per migliorare. Gli algoritmi che sfruttano questa proprietà sono <em>algoritmi basati su gradienti</em>, e tendono a essere molto più efficienti in quanto non provano perturbazioni a caso.</p> <p>Un’altra possibile alternativa sono gli algoritmi di <em>ottimizzazione bayesiana</em>. Come gli algoritmi genetici, anche loro esplorano lo spazio di stato casualmente perturbando la soluzione. Tuttavia, la perturbazione è applicata in maniera intelligente, in quanto in maniera sistematica provano a esplorare soluzioni mai esplorate senza però allontanarsi da soluzioni promettenti. Oltre ad aver bisogno della differenziabilità, questi algoritmi ipotizzano anche che soluzioni molto simili hanno fitness simile, senza grandi discontinuità.</p>]]></content><author><name></name></author><category term="animations"/><category term="matematica"/><category term="code"/><summary type="html"><![CDATA[Modellare utilizzando algoritmi genetici, applicato al problema delle 8 regine]]></summary></entry></feed>